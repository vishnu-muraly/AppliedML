{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/326029031\n",
      "\n",
      "Deep learning and its applications to machine health monitoring\n",
      "\n",
      "Article  in  Mechanical Systems and Signal Processing · January 2019\n",
      "\n",
      "DOI: 10.1016/j.ymssp.2018.05.050\n",
      "\n",
      "CITATIONS\n",
      "11\n",
      "\n",
      "6 authors, including:\n",
      "\n",
      "Rui Zhao\n",
      "Nanyang Technological University\n",
      "\n",
      "25 PUBLICATIONS   403 CITATIONS   \n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "Zhenghua Chen\n",
      "Nanyang Technological University\n",
      "\n",
      "22 PUBLICATIONS   385 CITATIONS   \n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "READS\n",
      "1,382\n",
      "\n",
      "Ruqiang Yan\n",
      "Xi'an Jiaotong University\n",
      "\n",
      "155 PUBLICATIONS   3,169 CITATIONS   \n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "Peng Wang\n",
      "Case Western Reserve University\n",
      "\n",
      "41 PUBLICATIONS   188 CITATIONS   \n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "Some of the authors of this publication are also working on these related projects:\n",
      "\n",
      "Deep Learning for Dynamical System Estimation and Prediction View project\n",
      "\n",
      "Rolling Element Bearing Diagnostics and Prognostics View project\n",
      "\n",
      "All content following this page was uploaded by Robert X. Gao on 07 November 2018.\n",
      "\n",
      "The user has requested enhancement of the downloaded file.\n",
      "\n",
      "\f",
      "Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "Contents lists available at ScienceDirect\n",
      "\n",
      "Mechanical Systems and Signal Processing\n",
      "\n",
      "j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / y m s s p\n",
      "\n",
      "Deep learning and its applications to machine health\n",
      "monitoring\n",
      "Rui Zhao a, Ruqiang Yan a,⇑\n",
      "\n",
      ", Zhenghua Chen b, Kezhi Mao b, Peng Wang c, Robert X. Gao c\n",
      "\n",
      "a School of Mechanical Engineering, Xi’an Jiaotong University, China\n",
      "b School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore\n",
      "c Department of Mechanical and Aerospace Engineering, Case Western Reserve University, United States\n",
      "\n",
      "a r t i c l e\n",
      "\n",
      "i n f o\n",
      "\n",
      "a b s t r a c t\n",
      "\n",
      "Article history:\n",
      "Received 3 February 2018\n",
      "Received in revised form 18 April 2018\n",
      "Accepted 27 May 2018\n",
      "\n",
      "Keywords:\n",
      "Deep learning\n",
      "Machine health monitoring\n",
      "Big data\n",
      "\n",
      "Contents\n",
      "\n",
      "Since 2006, deep learning (DL) has become a rapidly growing research direction, redeﬁning\n",
      "state-of-the-art performances in a wide range of areas such as object recognition, image\n",
      "segmentation, speech recognition and machine translation. In modern manufacturing sys-\n",
      "tems, data-driven machine health monitoring is gaining in popularity due to the wide-\n",
      "spread deployment of low-cost sensors and their connection to the Internet. Meanwhile,\n",
      "deep learning provides useful tools for processing and analyzing these big machinery data.\n",
      "The main purpose of this paper is to review and summarize the emerging research work of\n",
      "deep learning on machine health monitoring. After the brief introduction of deep learning\n",
      "techniques, the applications of deep learning in machine health monitoring systems are\n",
      "reviewed mainly from the following aspects: Auto-encoder (AE) and its variants,\n",
      "Restricted Boltzmann Machines and its variants including Deep Belief Network (DBN)\n",
      "and Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN) and\n",
      "Recurrent Neural Networks (RNN). In addition, an experimental study on the performances\n",
      "of these approaches has been conducted, in which the data and code have been online.\n",
      "Finally, some new trends of DL-based machine health monitoring methods are discussed.\n",
      "Ó 2018 Elsevier Ltd. All rights reserved.\n",
      "\n",
      "1.\n",
      "2.\n",
      "\n",
      "Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\n",
      "Deep learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "Auto-encoders (AE) and its variants. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "2.1.\n",
      "2.1.1.\n",
      "Addition of sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "Addition of denoising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "2.1.2.\n",
      "2.1.3.\n",
      "Stacking structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n",
      "RBM and its variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n",
      "Deep belief network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n",
      "2.2.1.\n",
      "2.2.2.\n",
      "Deep Boltzmann Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n",
      "Convolutional neural network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n",
      "Convolution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "2.3.1.\n",
      "2.3.2.\n",
      "Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "\n",
      "2.2.\n",
      "\n",
      "2.3.\n",
      "\n",
      "⇑ Corresponding author.\n",
      "\n",
      "E-mail address: rqyan@ieee.org (R. Yan).\n",
      "\n",
      "https://doi.org/10.1016/j.ymssp.2018.05.050\n",
      "0888-3270/Ó 2018 Elsevier Ltd. All rights reserved.\n",
      "\n",
      "\f",
      "214\n",
      "\n",
      "3.\n",
      "\n",
      "4.\n",
      "\n",
      "5.\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "Recurrent neural network. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "2.4.\n",
      "2.5.\n",
      "Optimization methods for neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221\n",
      "Applications of deep learning in machine health monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221\n",
      "3.1.\n",
      "AE and its variants for machine health monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221\n",
      "RBM and its variants for machine health monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n",
      "3.2.\n",
      "CNN for machine health monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224\n",
      "3.3.\n",
      "RNN for machine health monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\n",
      "3.4.\n",
      "Fault feature extraction for DL-based MHMS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n",
      "3.5.\n",
      "3.5.1.\n",
      "Time domain. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n",
      "Frequency domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n",
      "3.5.2.\n",
      "3.5.3.\n",
      "Time-frequency domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n",
      "Experimental evaluations of various DL techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n",
      "Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n",
      "4.1.\n",
      "4.2.\n",
      "Experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n",
      "Summary and future directions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n",
      "Acknowledgment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n",
      "References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "Industrial Internet of Things (IoT) and data-driven techniques have been revolutionizing manufacturing by enabling com-\n",
      "puter networks to gather the huge amount of data from connected machines and turn the big machinery data into actionable\n",
      "information [1–3]. As a key component in modern manufacturing system, machine health monitoring has fully embraced the\n",
      "big data revolution. Compared to top-down modeling provided by the traditional physics-based models [4–6], data-driven\n",
      "machine health monitoring systems offer a new paradigm of bottom-up solution for detection of faults after the occurrence\n",
      "of certain failures (diagnosis) and predictions of the future working conditions and the remaining useful life (prognosis) [1,7].\n",
      "As we all know, the complex and noisy working condition hinders the construction of physical models, which make the mod-\n",
      "elling of complex dynamic systems very difﬁcult [8,9]. Most of these physics-based models are unable to be updated with\n",
      "on-line measured data, which limits their effectiveness and ﬂexibility. On the other hand, with signiﬁcant development of\n",
      "sensors, sensor networks and computing systems, data-driven machine health monitoring models have become more and\n",
      "more attractive. To extract useful knowledge and make appropriate decisions from big data, machine learning techniques\n",
      "have been regarded as a powerful solution. As the hottest subﬁeld of machine learning, deep learning is able to act as a bridge\n",
      "connecting big machinery data and intelligent machine health monitoring.\n",
      "\n",
      "As a branch of machine learning, deep learning attempts to model hierarchical representations behind data and classify\n",
      "(predict) patterns via stacking multiple layers of information processing modules in hierarchical architectures. Recently,\n",
      "deep learning has been successfully adopted in various areas such as computer vision, automatic speech recognition, natural\n",
      "language processing, audio recognition and bioinformatics [10–13]. In fact, deep learning is not a new idea, which even dates\n",
      "back to the 1940s [14,15]. The popularity of deep learning today can be contributed to the following aspects:\n",
      "\n",
      "* Increasing Computing Power: the advent of graphics processor unit (GPU), the lowered cost of hardware, the better soft-\n",
      "ware infrastructure and the faster network connectivity all reduce the required running time of deep learning algorithms\n",
      "signiﬁcantly. For example, as reported in [16], the time required to learn a four-layer Deep Belief Network with 100 mil-\n",
      "lion free parameters can be reduced from several weeks to around a single day.\n",
      "\n",
      "* Increasing Data Size: there is no doubt that the era of Big Data is coming. Our activities are almost all digitized, recorded by\n",
      "computers and sensors, connected to Internet, and stored in cloud. As pointed out in [1] that in industry-related appli-\n",
      "cations such as industrial informatics and electronics, almost 1000 exabytes are generated per year and a 20-fold increase\n",
      "can be expected in the next ten years. The study in [3] predicts that 30 billion devices will be connected by 2020. There-\n",
      "fore, the huge amount of data is able to offset the complexity increase behind deep learning and improve its generaliza-\n",
      "tion capability.\n",
      "\n",
      "* Advanced Deep Learning Research: the ﬁrst breakthrough of deep learning is the pre-training method in an unsupervised\n",
      "way [17], where Hinton proposed to pre-train one layer at a time via restricted Boltzmann machine (RBM) and then ﬁne-\n",
      "tune using backpropagation in 2007. This has been proven to be effective to train multi-layer neural networks.\n",
      "\n",
      "Considering the capability of deep learning to address large-scale data and learn multi-scale/multi-level/hierarchical rep-\n",
      "resentation, deep learning can be a powerful and effective solution for machine health monitoring systems (MHMS). Conven-\n",
      "tional data-driven MHMS usually consists of the following key parts: hand-crafted feature design, feature extraction/\n",
      "selection and model training. The right set of features are designed, and then provided to some shallow machine learning\n",
      "algorithms including Support Vector Machines (SVM), Naive Bayes (NB), logistic regression [18–20]. It is shown that the rep-\n",
      "resentation of the data, which is provided to the machine learning algorithms, limits the performance [21]. However, it is\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "215\n",
      "\n",
      "difﬁcult to design appropriate features and perform feature selection. To alleviate this issue, feature extraction/selection\n",
      "methods, which can be regarded as a kind of information fusion, are performed between hand-crafted feature design and\n",
      "classiﬁcation/regression models [22–24]. However, manually designing features for a complex domain requires a great deal\n",
      "of human labor and cannot be updated on-line. Additionally, it also requires a signiﬁcant amount of expertise from the prac-\n",
      "titioner, which is not always available. At last, the above three modules including feature design, feature extraction/selection\n",
      "and model training cannot be jointly optimized which may hinder the ﬁnal performance of the whole system. Deep learning\n",
      "based MHMS (DL-based MHMS) aim to extract hierarchical representations from input data by building deep neural net-\n",
      "works with multiple layers of non-linear transformations. Intuitively, one layer operation can be regarded as a transforma-\n",
      "tion from input values to output values. Therefore, the application of one layer can learn a new representation of the input\n",
      "data and then, the stacking structure of multiple layers can enable MHMS to learn complex concepts out of simple concepts\n",
      "that can be constructed from raw input. In addition, DL-based MHMS achieve an end-to-end system, which can automati-\n",
      "cally learn internal representations from raw input and predict targets. Compared to conventional data driven MHMS,\n",
      "DL-based MHMS do not require extensive human labor and knowledge for hand-crafted feature design. All model parame-\n",
      "ters including feature module and pattern classiﬁcation/regression module can be trained jointly. Therefore, DL-based mod-\n",
      "els can be applied to addressing machine health monitoring in a very general way. For example, it is possible that the model\n",
      "trained for fault diagnosis problem can be used for prognosis by only replacing the top softmax layer with a linear regression\n",
      "layer requiring some re-training [25]. The comparison between conventional data-driven MHMS and DL-based MHMS is\n",
      "given in Table 1. A high-level illustration of the principles behind these three kinds of MHMS discussed above is shown\n",
      "in Fig. 1.\n",
      "\n",
      "Deep learning models have several variants such as Auto-encoders [26], Deep Belief Network [27], Deep Boltzmann\n",
      "Machines [28], Convolutional Neural Networks [29] and Recurrent Neural Networks [30]. During recent years, various\n",
      "researchers have demonstrated success of these deep learning models in the application of machine health monitoring. This\n",
      "paper attempts to provide a wide overview on these latest DL-based MHMS works that impact the state-of-the-art technolo-\n",
      "gies. Compared to these frontiers of deep learning including Computer Vision and Natural Language Processing, machine\n",
      "health monitoring community is catching up and has witnessed an emerging research. Therefore, the purpose of this study\n",
      "is to present researchers and engineers in the area of machine health monitoring system, a global view of this hot and active\n",
      "topic, and help them to acquire basic knowledge, quickly apply deep learning models and develop novel DL-based MHMS.\n",
      "The remainder of this paper is organized as follows. The basic information on these deep learning models mentioned above\n",
      "are given in Section 2. Then, Section 3 reviews applications of deep learning models on machine health monitoring. In Sec-\n",
      "tion 4, an experimental study has been conducted in a tool wear prediction task. Finally, Section 5 gives a brief summary of\n",
      "\n",
      "Table 1\n",
      "Summary on comparison between conventional data-driven MHMS and DL-based MHMS.\n",
      "\n",
      "MHMS\n",
      "\n",
      "Conventional Data-driven Methods\n",
      "\n",
      "Expert knowledge and extensive human labor required for Hand-crafted features\n",
      "Individual modules are trained step-by-step\n",
      "Unable to model large-scale data\n",
      "\n",
      "Deep Learning Methods\n",
      "\n",
      "End-to-end structure without hand-crafted features\n",
      "All parameters are trained jointly\n",
      "Suitable for large-scale data\n",
      "\n",
      "Physical-based\n",
      "\n",
      "MHMS\n",
      "\n",
      "Conventional Data-driven\n",
      "\n",
      "MHMS\n",
      "\n",
      "Deep Learning based\n",
      "\n",
      "MHMS\n",
      "\n",
      "Monitored Machines\n",
      "\n",
      "Monitored Machines\n",
      "\n",
      "Monitored Machines\n",
      "\n",
      "N\n",
      "/\n",
      "x\n",
      "F\n",
      "\n",
      "N\n",
      "/\n",
      "x\n",
      "F\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "-5\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "-5\n",
      "\n",
      "N\n",
      "/\n",
      "x\n",
      "F\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "-5\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "Sample Data\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "x 104\n",
      "\n",
      "Data Acquisition\n",
      "\n",
      "Hand-designed Physical \n",
      "\n",
      "Models\n",
      "\n",
      "Targets\n",
      "\n",
      "RMS /Variance/ \n",
      "\n",
      "Maximum /Minimum/ \n",
      "Skewness /Kurtosis/ \n",
      "Peak/ Kurtosis Index/ \n",
      "\n",
      "Wavelet Energy \n",
      "\n",
      "Hand-designed Features\n",
      "\n",
      "PCA\n",
      "\n",
      "LDA\n",
      "\n",
      "ICA\n",
      "\n",
      "Feature Selection/\n",
      "Extraction Methods\n",
      "\n",
      "Neural \n",
      "Networks\n",
      "\n",
      "Support Vector \n",
      "\n",
      "Machines \n",
      "\n",
      "Random Forest\n",
      "\n",
      "Model Training\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "Sample Data\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "x 104\n",
      "\n",
      "Data Acquisition\n",
      "\n",
      "Targets\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "Sample Data\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "x 104\n",
      "\n",
      "Data Acquisition\n",
      "\n",
      "From Simple Features to Abstract Features\n",
      "\n",
      "Softmax/\n",
      "Regression\n",
      "\n",
      "Targets\n",
      "\n",
      "Fig. 1. Frameworks showing three different MHMS including Physical Models, Conventional Data-driven Models and Deep Learning Models. Shaded boxes\n",
      "denote data-driven components.\n",
      "\n",
      "\f",
      "216\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "the recent achievements of DL-based MHMS and discusses some potential trends of deep learning in machine health\n",
      "monitoring.\n",
      "\n",
      "As a survey paper, a comprehensive review of recent research efforts on deep learning based machine health monitoring\n",
      "system is given to present the whole picture of state-of-art related research for readers and foster the potential innovation in\n",
      "this ﬁeld. In addition, implementations of several classical DL-based MHMS are public with which researchers/engineers can\n",
      "quickly understand and contribute to the ﬁeld of DL-based MHMS research area.\n",
      "\n",
      "2. Deep learning\n",
      "\n",
      "Originated from artiﬁcial neural network, deep learning is a branch of machine learning which is featured by multiple\n",
      "non-linear processing layers and tries to learn hierarchical representations of data. Up to date, there are various deep learn-\n",
      "ing architectures and this research topic is fast-growing, in which new models are being developed even every week. The\n",
      "community is quite open and there are a number of deep learning tutorials and books of good-quality [31,32]. Therefore,\n",
      "only a brief introduction to some major deep learning techniques that have been applied in machine health monitoring is\n",
      "given. In the following, four deep architectures including Auto-encoders, RBM, CNN, RNN and their corresponding variants\n",
      "are reviewed.\n",
      "\n",
      "2.1. Auto-encoders (AE) and its variants\n",
      "\n",
      "As a feed-forward neural network, auto-encoder consists of two phases including encoder and decoder, which is designed\n",
      "to learn a new representation of the data by trying to reconstruct the input data. Encoder takes an input x and transforms it\n",
      "to a hidden representation h via a non-linear mapping as follows:\n",
      "\n",
      "h ¼ uðWx þ bÞ\n",
      "\n",
      "ð1Þ\n",
      "where u is a non-linear activation function. The commonly used activation functions include softmax, relu, tanh, sigmoid and\n",
      "so on. Then, decoder maps the hidden representation back to the original representation in a similar way as follows:\n",
      "\n",
      "z ¼ uðW0h þ b0Þ\n",
      "\n",
      "ð2Þ\n",
      "Model parameters including h ¼ ½W; b; W0; b0 are optimized to minimize the reconstruction error between z ¼ f hðxÞ and x.\n",
      "One commonly adopted measure for the average reconstruction error over a collection of N data samples is squared error\n",
      "and the corresponding optimization problem can be written as follows:\n",
      "\n",
      "XN\n",
      "kxi \u0000 f hðxiÞk2\n",
      "\n",
      "2\n",
      "\n",
      "min\n",
      "\n",
      "h\n",
      "\n",
      "1\n",
      "N\n",
      "\n",
      "i\n",
      "\n",
      "where xi is the i-th sample. It is clearly shown that AE can be trained in an unsupervised way. The hidden representation h\n",
      "can be regarded as a more abstract and meaningful representation for data sample x.\n",
      "\n",
      "2.1.1. Addition of sparsity\n",
      "\n",
      "To prevent the learned transformation to be the identity one and regularize auto-encoders, the sparsity constraint is\n",
      "\n",
      "imposed on the hidden units [33]. The corresponding optimization function is updated as:\n",
      "\n",
      "XN\n",
      "kxi \u0000 f hðxiÞk2\n",
      "\n",
      "2 þ b\n",
      "\n",
      "Xm\n",
      "KLðpjjpjÞ\n",
      "\n",
      "i\n",
      "\n",
      "j\n",
      "\n",
      "min\n",
      "\n",
      "h\n",
      "\n",
      "1\n",
      "N\n",
      "\n",
      "  !\n",
      "\n",
      "KLðpjjpjÞ ¼ p log\n",
      "\n",
      "þ ð1 \u0000 pÞ log\n",
      "\n",
      "p\n",
      "pj\n",
      "\n",
      " \n",
      "\n",
      "!\n",
      "\n",
      "1 \u0000 p\n",
      "1 \u0000 pj\n",
      "\n",
      "where m is the hidden layer size, the second term is the summation of the KL-divergence over the hidden units and b is a\n",
      "controlling weight for the sparsity penalty term. The KL-divergence on j-th hidden neuron is given as:\n",
      "\n",
      "where p is the predeﬁned mean activation target and pj is the average activation of the j-th hidden neuron over the entire\n",
      "dataset. Given a small p, the addition of sparsity constraint can lead the learned hidden representation to be a sparse rep-\n",
      "resentation. Therefore, the variant of AE is named as sparse auto-encoder.\n",
      "\n",
      "2.1.2. Addition of denoising\n",
      "\n",
      "Different from conventional AE, denoising AE takes a corrupted version of data as input and is trained to reconstruct/de-\n",
      "noise the clean input x from its corrupted sample ~x. The most commonly adopted noise is dropout noise/binary masking\n",
      "noise, which randomly sets a fraction of the input features to be zero [26]. The variant of AE is denoising auto-encoder\n",
      "(DA), which can learn more robust representation and prevent it from learning the identity transformation.\n",
      "\n",
      "ð3Þ\n",
      "\n",
      "ð4Þ\n",
      "\n",
      "ð5Þ\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "217\n",
      "\n",
      "2.1.3. Stacking structure\n",
      "\n",
      "Several DA can be stacked together to form a deep network and learn representations by feeding the outputs of the l-th\n",
      "\n",
      "layer as inputs to the ðl þ 1Þ-th layer [26]. And the training is done one layer greedily at a time.\n",
      "\n",
      "Since auto-encoder can be trained in an unsupervised way, auto-encoder, especially stacked denoising auto-encoder\n",
      "(SDA), can provide an effective pre-training solution via initializing the weights of deep neural network (DNN) to train\n",
      "the model. After layer-wise pre-training of SDA, the parameters of auto-encoders can be set to the initialization for all the\n",
      "hidden layers of DNN. Then, the supervised ﬁne-tuning is performed to minimize prediction error on a labeled training data.\n",
      "Usually, a softmax/regression layer is added on top of the network to map the output of the last layer in AE to targets. The\n",
      "whole process is shown in Fig. 2. The pre-training protocol based on SDA can make DNN models have better convergence\n",
      "capability compared to arbitrary random initialization. It should be noted that training deep neural networks often suffers\n",
      "from gradient vanishing/exploding problems due to these commonly adopted tanh or sigmoid nonlinear activation functions.\n",
      "Therefore, unsupervised training enabled by AE is meaningful and powerful. However, relu activation relieved this problem,\n",
      "which was proposed in 2012. Supervised training of deep neural networks such as deep convolutional neural network and\n",
      "recurrent neural network became possible (see Fig. 3).\n",
      "\n",
      "h1\n",
      "\n",
      "x\n",
      "\n",
      "h1\n",
      "\n",
      "x\n",
      "\n",
      "h2\n",
      "\n",
      "h1\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "h2\n",
      "\n",
      "h1\n",
      "\n",
      "x\n",
      "\n",
      "Unsupervised Layer-wise Pretraining    \n",
      "\n",
      "Supervised Fine-tuning\n",
      "\n",
      "(a) SDA-based DNN\n",
      "\n",
      "h2\n",
      "\n",
      "h1\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "h2\n",
      "\n",
      "h1\n",
      "\n",
      "x\n",
      "\n",
      "Unsupervised Layer-wise Pretraining    \n",
      "\n",
      "Supervised Fine-tuning\n",
      "\n",
      "(b) DBN-based DNN\n",
      "\n",
      "Fig. 2. Illustrations for unsupervised pre-training and supervised ﬁne-tuning of SAE-DNN (a) and DBN-DNN (b).\n",
      "\n",
      "RBM\n",
      "\n",
      "DBN\n",
      "\n",
      "DBM\n",
      "\n",
      "Fig. 3. Frameworks showing RBM, DBN and DBM. Shaded boxes denote hidden units.\n",
      "\n",
      "\f",
      "218\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "2.2. RBM and its variants\n",
      "\n",
      "As a special type of Markov random ﬁeld, restricted Boltzmann machine (RBM) is a two-layer neural network forming a\n",
      "bipartite graph that consists of two groups of units including visible units v and hidden units h under the constraint that\n",
      "there exists a symmetric connection between visible units and hidden units and there are no connections between nodes\n",
      "with a group.\n",
      "\n",
      "Given the model parameters h ¼ ½W; b; a, the energy function can be given as:\n",
      "\n",
      "Eðv; h; hÞ ¼ \u0000\n",
      "\n",
      "XI\n",
      "XJ\n",
      "wijv ihj \u0000\n",
      "\n",
      "XI\n",
      "biv i \u0000\n",
      "\n",
      "i¼1\n",
      "\n",
      "j¼1\n",
      "\n",
      "i¼1\n",
      "\n",
      "XJ\n",
      "\n",
      "j¼1\n",
      "\n",
      "ajhj\n",
      "\n",
      "that wij is the connecting weight between visible unit v i, whose total number is I and hidden unit hj whose total number is\n",
      "J; bi and aj denote the bias terms for visible units and hidden units, respectively. The joint distribution over all the units is\n",
      "calculated based on the energy function Eðv; h; hÞ as:\n",
      "\n",
      "P\n",
      "h;vexpð\u0000Eðv; h; hÞÞ is the partition function or normalization factor. Then, the conditional probabilities of hidden\n",
      "\n",
      "Z\n",
      "\n",
      "where Z ¼\n",
      "and visible units h and v can be calculated as:\n",
      "\n",
      "ð6Þ\n",
      "\n",
      "ð7Þ\n",
      "\n",
      "!\n",
      "\n",
      "ð8Þ\n",
      "\n",
      "ð9Þ\n",
      "\n",
      "pðv; h; hÞ ¼ expð\u0000Eðv; h; hÞÞ\n",
      "\n",
      "pðhj ¼ 1jv; hÞ ¼ d\n",
      "\n",
      "pðv i ¼ 1jv; hÞ ¼ d\n",
      "\n",
      "!\n",
      "\n",
      " \n",
      "\n",
      "XI\n",
      "wijv i þ aj\n",
      " \n",
      "XJ\n",
      "wijhj þ bi\n",
      "\n",
      "i¼1\n",
      "\n",
      "j¼1\n",
      "\n",
      "where d is deﬁned as a logistic function, i.e., dðxÞ ¼ 1\n",
      "is done through a method called contrastive divergence (CD) [34].\n",
      "\n",
      "1þexpðxÞ. RBM is trained to maximize the joint probability. The learning of W\n",
      "\n",
      "2.2.1. Deep belief network\n",
      "\n",
      "Deep belief network (DBN) can be constructed by stacking multiple RBMs, where the output of the l-th layer (hidden\n",
      "units) is used as the input of the ðl þ 1Þ-th layer (visible units) [35]. Similar to SDA, DBN can be trained in a greedy layer-\n",
      "wise unsupervised way. After pre-training, the parameters of this deep architecture can be further ﬁne-tuned with respect\n",
      "to a proxy for the DBN log-likelihood, or with respect to labels of training data by adding a softmax layer as the top layer,\n",
      "which is shown in Fig. 2(b).\n",
      "\n",
      "2.2.2. Deep Boltzmann Machine\n",
      "\n",
      "Deep Boltzmann machine (DBM) can be regarded as a deep structured RMBs where hidden units are grouped into a hier-\n",
      "archy of layers instead of a single layer [28]. Following the RMB’s connectivity constraint, there is only full connectivity\n",
      "between subsequent layers and no connections within layers or between non-neighbouring layers are allowed. The main\n",
      "difference between DBN and DBM lies that DBM is fully undirected graphical model, while DBN is mixed directed/undirected\n",
      "one. Different from DBN that can be trained layer-wisely, DBM is trained as a joint model. Therefore, the training of DBM is\n",
      "more computationally expensive than that of DBN.\n",
      "\n",
      "2.3. Convolutional neural network\n",
      "\n",
      "Convolutional neural networks (CNNs) were ﬁrstly proposed by LeCun [36] for image processing, which is featured by\n",
      "two key properties: spatially shared weights and spatial pooling. CNN models have shown their success in various computer\n",
      "vision applications [36–38] where input data are usually 2D data. CNN has also been introduced to address sequential data\n",
      "including Natural Language Processing and Speech Recognition [39,40].\n",
      "\n",
      "CNN aims to learn abstract features by alternating and stacking convolutional layers and pooling layers. In CNN, the con-\n",
      "volutional layers (convolutional kernels) convolve multiple local ﬁlters with raw input data and generate translation-\n",
      "invariant local features and the subsequent pooling layers extract features with a ﬁxed-length over sliding windows of\n",
      "the raw input data following several rules such as average, max and so on. Considering that 2D-CNN has been illustrated\n",
      "extensively in previous research compared to 1D-CNN, here, only the mathematical details behind 1D-CNN is given as\n",
      "follows:\n",
      "\n",
      "Firstly, we assume that the input sequential data is x ¼ ½x1; . . . ; xT that T is the length of the sequence and xi 2 Rd at each\n",
      "\n",
      "time step.\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "219\n",
      "\n",
      "2.3.1. Convolution\n",
      "\n",
      "tion operation as follows:\n",
      "\n",
      "the dot product between a ﬁlter vector u 2 Rmd and an concatenation vector representation xi:iþm\u00001 deﬁnes the convolu-\n",
      "\n",
      "ci ¼ uðu \u0001 xi:iþm\u00001 þ bÞ\n",
      "\n",
      "ð10Þ\n",
      "where \u0001 represents the dot product, b and u denote bias term and non-linear activation function, respectively. xi:iþm\u00001 is a m-\n",
      "length window starting from the i-th time step, which is described as:\n",
      "\n",
      "xi:iþm\u00001 ¼ xi \b xiþ1 \b . . . \b xiþm\u00001\n",
      "\n",
      "ð11Þ\n",
      "where \b is the concatenation operation of two vectors. As deﬁned in Eq. (10), the output scale ci can be regarded as the acti-\n",
      "vation of the ﬁlter u on the corresponding subsequence xi:iþm\u00001. By sliding the ﬁltering window from the beginning time step\n",
      "to the ending time step, a feature map as a vector can be given as follows:\n",
      "\n",
      "cj ¼ c1; c2; . . . ; cl\u0000mþ1\n",
      "\n",
      "½\n",
      "\n",
      "\n",
      "\n",
      "ð12Þ\n",
      "\n",
      "where the index j represents the j-th ﬁlter. It corresponds to multi-windows as fx1:m; . . . ; xl\u0000mþ1:lg.\n",
      "\n",
      "2.3.2. Pooling\n",
      "\n",
      "Pooling layer is able to reduce the length of the feature map, which can further minimize the number of model param-\n",
      "eters. These commonly adopted pooling operations include max and average pooling. In the following, max pooling is\n",
      "explained in details. The hyper-parameter of pooling layer is pooling length denoted as s. The max operation is to take a\n",
      "max over the s consecutive values in feature map cj.\n",
      "\n",
      "Then, the compressed feature vector can be obtained as:\n",
      "\n",
      "h\n",
      "\n",
      "i\n",
      "\n",
      "h ¼ h1; h2; . . . ; hl\u0000m\n",
      "s þ1\n",
      "\n",
      "ð13Þ\n",
      "where hj ¼ maxðcðj\u00001Þs; cðj\u00001Þsþ1; . . . ; cjs\u00001Þ. Then, via alternating the above two layers: convolution and max-pooling ones, fully\n",
      "connected layers and a softmax layer are usually added as the top layers to make predictions. To give a clear illustration, the\n",
      "framework for a one-layer CNN has been displayed in Fig. 4.\n",
      "\n",
      "2.4. Recurrent neural network\n",
      "\n",
      "As stated in [14], recurrent neural networks (RNNs) are the deepest of all neural networks, which can generate and\n",
      "address memories of arbitrary-length sequences of input patterns. RNN is able to build connections between units from a\n",
      "\n",
      "(cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)\n",
      "(cid:48)(cid:68)(cid:83)(cid:3)(cid:20)\n",
      "\n",
      "(cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)\n",
      "(cid:48)(cid:68)(cid:83)(cid:3)(cid:21)\n",
      "\n",
      "(cid:55)(cid:68)(cid:85)(cid:74)(cid:72)(cid:87)(cid:86)\n",
      "\n",
      "(cid:55)(cid:76)(cid:80)(cid:72)(cid:3)(cid:54)(cid:87)(cid:72)(cid:83)(cid:86)\n",
      "\n",
      "(cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)\n",
      "(cid:48)(cid:68)(cid:83)(cid:3)(cid:78)\n",
      "\n",
      "(cid:38)(cid:82)(cid:81)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)\n",
      "\n",
      "(cid:48)(cid:68)(cid:91)(cid:16)(cid:83)(cid:82)(cid:82)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)\n",
      "\n",
      "(cid:41)(cid:88)(cid:79)(cid:79)(cid:92)(cid:16)(cid:38)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)\n",
      "\n",
      "Fig. 4. Illustrations for one-layer CNN that contains one convolutional layer, one pooling layer, one fully-connected layer, and one softmax layer.\n",
      "\n",
      "(cid:41)(cid:79)(cid:68)(cid:87)(cid:87)(cid:72)(cid:81)\n",
      "\n",
      "(cid:54)(cid:82)(cid:73)(cid:87)(cid:80)(cid:68)(cid:91) (cid:47)(cid:68)(cid:92)(cid:72)(cid:85)\n",
      "\n",
      "\f",
      "220\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "directed cycle. Different from basic neural network: multi-layer perceptron that can only map from input data to target vec-\n",
      "tors, RNN is able to map from the entire history of previous inputs to target vectors in principle and allow a memory of pre-\n",
      "vious inputs to be kept in the network’s internal state. RNNs can be trained via backpropagation through time for supervised\n",
      "tasks with sequential input data and target outputs [41,30,42].\n",
      "\n",
      "RNN can address the sequential data using its internal memory, as shown in Fig. 5(a). The transition function deﬁned in\n",
      "each time step t takes the current time information xt and the previous hidden output ht\u00001 and updates the current hidden\n",
      "output as follows:\n",
      "\n",
      "ht ¼ Hðxt; ht\u00001Þ\n",
      "\n",
      "ð14Þ\n",
      "where H deﬁnes a nonlinear and differentiable transformation function. After processing the whole sequence, the hidden\n",
      "output at the last time step, i.e., hT, is the learned representation of the input sequential data whose length is T. A conven-\n",
      "tional Multilayer perceptron (MLP) is added on top to map the obtained representation hT to targets.\n",
      "\n",
      "ht ¼ uðWxt þ Hht\u00001 þ bÞ\n",
      "\n",
      "Various transition functions can lead to various RNN models. The most simple one is vanilla RNN that is given as follows:\n",
      "ð15Þ\n",
      "where W and H denote transformation matrices and b is the bias vector. And u denotes the nonlinear activation function\n",
      "such as sigmoid and tanh functions. Due to the vanishing gradient problem during backpropagation for model training,\n",
      "vanilla RNN may not capture long-term dependencies. Therefore, Long-short term memory (LSTM) and gated recurrent units\n",
      "(GRU) were presented to prevent backpropagated errors from vanishing or exploding [43–47]. The core idea behind these\n",
      "advanced RNN variants is that gates are introduced to avoid the long-term dependency problem and enable each recurrent\n",
      "unit to adaptively capture dependencies of different time scales.\n",
      "\n",
      "Time Series \n",
      "\n",
      "X\n",
      "\n",
      "1\n",
      "\n",
      "X\n",
      "\n",
      "2\n",
      "\n",
      "X\n",
      "\n",
      "3\n",
      "\n",
      "(a) Basic RNN\n",
      "\n",
      "–1TX\n",
      "\n",
      "2\n",
      "\n",
      "h\n",
      "1\n",
      "\n",
      "h\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "h\n",
      "\n",
      "2\n",
      "\n",
      "h\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "Th\n",
      "\n",
      "TX\n",
      "\n",
      "3\n",
      "\n",
      "Th\n",
      "\n",
      "2\n",
      "\n",
      "Th\n",
      "\n",
      "1\n",
      "\n",
      "Th\n",
      "\n",
      "Time Series \n",
      "\n",
      "X\n",
      "\n",
      "1\n",
      "\n",
      "X\n",
      "\n",
      "2\n",
      "\n",
      "X\n",
      "\n",
      "3\n",
      "\n",
      "(b) Stacked RNN\n",
      "\n",
      "–1TX\n",
      "\n",
      "TX\n",
      "Th\n",
      "\n",
      "Concatenated\n",
      "\n",
      "Th\n",
      "\n",
      "Time Series \n",
      "\n",
      "X\n",
      "\n",
      "1\n",
      "\n",
      "X\n",
      "\n",
      "2\n",
      "\n",
      "X\n",
      "\n",
      "3\n",
      "\n",
      "(c) Bidirectional RNN\n",
      "\n",
      "–1TX\n",
      "\n",
      "TX\n",
      "\n",
      "Fig. 5. Illustrations of normal RNN, stacked RNN and bidirectional RNN.\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "221\n",
      "\n",
      "Besides these proposed advanced transition functions such as LSTMs and GRUs, multi-layer and bi-directional recurrent\n",
      "structure can increase the model capacity and ﬂexibility. As shown in Fig. 5(b), multi-layer structure can enable the hidden\n",
      "output of one recurrent layer to be propagated through time and used as the input data to the next recurrent layer. The bidi-\n",
      "rectional recurrent structure is able to process the sequence data in two directions including forward and backward ways\n",
      "with two separate hidden layers, which is illustrated in Fig. 5(c). The following equations deﬁne the corresponding hidden\n",
      "layer function and the ! and   denote forward and backward processes, respectively.\n",
      "\n",
      "!\n",
      "h\n",
      " \n",
      "h\n",
      "\n",
      "t ¼ H\n",
      "t ¼ H\n",
      "\n",
      "!\n",
      "!ðxt; h\n",
      "t\u00001Þ;\n",
      " \n",
      " ðxt; h\n",
      "tþ1Þ:\n",
      "\n",
      "Then, the ﬁnal vector hT is the concatenated vector of the outputs of forward and backward processes as follows:\n",
      "\n",
      "!\n",
      "hT ¼ h\n",
      "\n",
      " \n",
      "T \b h\n",
      "1\n",
      "\n",
      "2.5. Optimization methods for neural networks\n",
      "\n",
      "ð16Þ\n",
      "\n",
      "ð17Þ\n",
      "\n",
      "To learn optimal parameters for neural network models, gradient descent is the most widely used method. Gradient des-\n",
      "cent is performed to minimize an objective function JðhÞ by updating the parameters h in the opposite direction of the gra-\n",
      "dient of the object function rJðhÞ w.r.t to the parameters, in which the learning rate is used to control the size of the steps to\n",
      "reach a minimum. An overview of various gradient descent methods optimization for deep learning models is given here\n",
      "[48]. However, one tricky point lies that the weight initialization will inﬂuence the convergence so that it is necessary to\n",
      "select appropriate model parameters initialization scheme [49].\n",
      "\n",
      "3. Applications of deep learning in machine health monitoring\n",
      "\n",
      "The conventional MLP has been applied in the ﬁeld of machine health monitoring for many years [50–53]. The deep learn-\n",
      "ing techniques have recently been applied to a large number of machine health monitoring systems. The layer-by-layer pre-\n",
      "training of DNN based on AE or RBM can facilitate the training of DNN and improve its discriminative power to characterize\n",
      "machinery data. CNN and RNN provide more advanced and complex composition mechanism to learn representations from\n",
      "machinery data. In these DL-based MHMS systems, the top layer normally represents the targets. For diagnosis where targets\n",
      "are discrete values, softmax layer is applied. For prognosis with continuous targets, liner regression layer is added. What is\n",
      "more, the end-to-end structure enables DL-based MHMS to be constructed with less human labor and expert knowledge,\n",
      "therefore these models are not limited to speciﬁc equipment or domain. In the following, a brief survey of DL-based MHMS\n",
      "is presented in these above four DL architectures: AE, RBM, CNN and RNN.\n",
      "\n",
      "3.1. AE and its variants for machine health monitoring\n",
      "\n",
      "AE models, especially stacked DA, can learn representations from machinery data in an automatic way. Sun et al. pro-\n",
      "posed a one layer AE-based neural network to classify induction motor faults [54]. Due to the limited size of training data,\n",
      "they focused on preventing overﬁtting. Not only the number of hidden layer was set to 1, but also dropout technique that\n",
      "masks portions of output neurons randomly was applied on the hidden layer. The whole model has been shown in Fig. 6. The\n",
      "majority of proposed models are based on deep architectures by stacking multiple auto-encoders. For example, Lu et al. pre-\n",
      "sented a detailed empirical study of stacked denoising autoencoders with three hidden layers for fault diagnosis of rotating\n",
      "machinery components [55]. Speciﬁcally, in their experiments including single working condition that training and testing\n",
      "data share one operation condition and cross working conditions that training and testing data are sampled from two dif-\n",
      "ferent operation conditions, the effectiveness of deep architecture, sparsity constraint and denoising operation in the SDA\n",
      "model were evaluated. They recommended that three hidden layers with sparsity criterion of 0.15 and destruction level\n",
      "of 0.25 is optimal. In [56], different structures of a two-layer SAE-based DNN were designed by varying hidden layer size\n",
      "and its masking probability, and evaluated for their performances in fault diagnosis.\n",
      "\n",
      "In these above works, the input features to AE models are raw sensory time-series. Therefore, the input dimensionality is\n",
      "always over hundred, even one thousand. The possible high dimensionality may lead to some potential concerns such as\n",
      "heavy computation cost and overﬁting caused by huge model parameters. Therefore, some researchers focused on AE models\n",
      "built upon features extracted from raw input. Jia et al. fed the frequency spectra of time-series data into SAE for rotating\n",
      "machinery diagnosis [57], considering the frequency spectra is able to demonstrate how their constitutive components\n",
      "are distributed with discrete frequencies and may be more discriminative over the health conditions of rotating machinery.\n",
      "In [58], Sun et al. utilized compressed sensing techniques to extract low-dimensional features from raw time-series signal as\n",
      "input features into SAE-DNN models. In [59], Zhou et al. proposed three cascaded SAE-DNNs that each module is for mode\n",
      "partition classiﬁcation, fault source location classiﬁcation and fault severity recognition, respectively. The input features are\n",
      "frequency coefﬁcients based on Fast Fourier Transform. Tan et al. used digital wavelet frame and nonlinear soft threshold\n",
      "method to process the vibration signal and built a SAE on the preprocessed signal for roller bearing fault diagnosis [60].\n",
      "\n",
      "\f",
      "222\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "..\n",
      "\n",
      "Fine-tune\n",
      "\n",
      "Updating \n",
      "weight\n",
      "\n",
      "TW\n",
      "\n",
      "W\n",
      "\n",
      "dropout\n",
      "\n",
      "Sparse \n",
      "penalty\n",
      "\n",
      "1W\n",
      "\n",
      "W\n",
      "\n",
      "Diagnosis\n",
      "Out~Y3 \n",
      "\n",
      "dropout \n",
      "decayed\n",
      "\n",
      "..\n",
      "\n",
      "1W\n",
      "\n",
      "W\n",
      "\n",
      "Corruption \n",
      "\n",
      "noise\n",
      "\n",
      "X1\n",
      "\n",
      "X2\n",
      "\n",
      "Y2\n",
      "\n",
      "X3\n",
      "\n",
      "Y3\n",
      "\n",
      "Sparse Auto-encoder\n",
      "\n",
      "Deep Neural Network (training)\n",
      "\n",
      "Testing for Classification\n",
      "\n",
      "Fig. 6. Illustrations of the proposed sparse autoencoder for induction motor fault diagnosis in [54].\n",
      "\n",
      "Zhu et al. proposed a SAE-based DNN for hydraulic pump fault diagnosis with input as frequency domain features after Four-\n",
      "ier transform [61]. In experiments, relu activation and dropout technique were analyzed and experimental results have\n",
      "shown to be effective in preventing gradient vanishing and overﬁting. In the work presented in [62], the normalized spec-\n",
      "trogram generated by STFT of sound signal was fed into two-layers SAE-based DNN for rolling bearing fault diagnosis. Gal-\n",
      "loway et al. built a two layer SAE-based DNN on spectrograms generated from raw vibration data for tidal turbine vibration\n",
      "fault diagnosis [63]. A SAE-based DNN with input as principal components of data extracted by principal component analysis\n",
      "was proposed for spacecraft fault diagnosis in [64]. Multi-domain statistical features including time domain features, fre-\n",
      "quency domain features and time–frequency domain features were fed into the SAE framework, which can be regarded\n",
      "as one kind of feature fusion [65]. Similarly, Verma et al. also used these three domains features to fed into a SAE-based\n",
      "DNN for fault diagnosis of air compressors [66] and Sohaib et al. proposed SAE-DNN model based on these three domains\n",
      "features for bearing fault diagnosis[67]. In [68], Chen et al. fed tri-domain features into SAE and adopted support vector\n",
      "machine as the ﬁnal classiﬁer. To overcome the issue of overﬁtting problem, Chen et al. adopted data augmentation tech-\n",
      "nique by adding Gaussian noise into training data [69].\n",
      "\n",
      "Except for these applied multi-domain features, multi-sensory data are also addressed by SAE models. Reddy et al. uti-\n",
      "lized SAE to learn representation on raw time series data from multiple sensors for anomaly detection and fault disambigua-\n",
      "tion in ﬂight data. To address multi-sensory data, synchronized windows were ﬁrstly traversed over multi-modal time series\n",
      "with overlap, and then windows from each sensor were concatenated as the input to the following SAE [70]. In [71], SAE was\n",
      "leveraged for multi-sensory data fusion and the followed DBN was adopted for bearing fault diagnosis, which achieved\n",
      "promising results. The statistical features in time domain and frequency domain extracted from the vibration signals of dif-\n",
      "ferent sensors were adopted as inputs to a two-layer SAE with sparsity constraint neural networks. The learned representa-\n",
      "tions were fed into a deep belief network for pattern classiﬁcation.\n",
      "\n",
      "In addition, some variants of the conventional SAE were proposed or introduced for machine health monitoring. In [72],\n",
      "Thirukovalluru et al. proposed a two-phase framework that SAE only learns representation and other standard classiﬁers\n",
      "such as SVM and random forest perform classiﬁcation. Speciﬁcally, in SAE module, handcrafted features based on FFT and\n",
      "wavelet packet transform (WPT) were fed into SAE-based DNN. After pre-training and supervised ﬁne-tuning which includes\n",
      "two separated procedures: softmax-based and Median-based ﬁne-tuning methods, the extensive experiments on ﬁve data-\n",
      "sets including air compressor monitoring, drill bit monitoring, bearing fault monitoring and steel plate monitoring demon-\n",
      "strated the generalization capability of DL-based machine health monitoring systems. Wang et al. proposed a novel\n",
      "continuous sparse auto-encoder (CSAE) as an unsupervised feature learning for transformer fault recognition [73]. Different\n",
      "from conventional sparse AE, their proposed CSAE added the stochastic unit into activation function of each visible unit as:\n",
      "ð18Þ\n",
      "\n",
      "X\n",
      "wijxi þ ai þ rNjð0; 1ÞÞ\n",
      "\n",
      "sj ¼ u\n",
      "\n",
      "jð\n",
      "\n",
      "i\n",
      "\n",
      "where sj is the output corresponding to the input xi; wij and ai denote model parameters, u\n",
      "j represents the activation func-\n",
      "tion and the last term rNjð0; 1ÞÞ is the added stochastic unit, which is a zero-mean Gaussian with variance r2. The incorpo-\n",
      "ration of stochastic unit is able to change the gradient direction and prevent overﬁtting. Mao et al. adopted a variant of AE\n",
      "named Extreme Learning Machine-based auto-encoder for bearing fault diagnosis, which is more efﬁcient than conventional\n",
      "SAE models without sacriﬁcing accuracies in fault diagnosis [74]. Different from AE that was trained via back-propagation,\n",
      "the transformation in encoder phase was randomly generated and the one in decoder phase was learned in a single step via\n",
      "least-squares ﬁt [75]. In [76], Jia et al. pointed out that two potential shortcomings behind traditional autoencoders such as\n",
      "similar features learning and shift variant properties hinder the performance on automatic feature extraction of mechanical\n",
      "signals. Therefore, they proposed normalized sparse autoencoder (NSAE) by adding rectiﬁed liner units as activation func-\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "223\n",
      "\n",
      "tion, abandoning bias and adopting L2 norm instead of KL divergence function in formulation of autoencoder. Then, a Local\n",
      "Connection Network (LCN) which takes the average value of NSAE’s outputs was adopted to extract shift-invariant features.\n",
      "To match the features of the complex signal, Shao et.al replaced the original MSE loss function with the maximum correlated\n",
      "entropy in their designed autoencoders and adopted artiﬁcial ﬁsh swarm algorithm to select its key parameters [77]. In their\n",
      "another work, an ensemble model of autoencoders with 15 different activation functions was proposed in [78], in which the\n",
      "ensemble scheme is based on majority voting with different weights. They also proposed a fusion scheme of two autoen-\n",
      "coders including denoising auto-encoder (DAE) and contractive auto-encoder (CAE) based on locality preserving projection\n",
      "(LPP) in [79]. Li et al. proposed a fully-connected winner-take-all autoencoder [80] (FCWTA) for bearing fault diagnosis. Their\n",
      "model’s novelty lies in two aspects: one is about lifetime virtualization and deep recognition for system fault classiﬁcation\n",
      "sparsity that only k largest activations of each hidden nodes among all samples in a mini-batch are kept and the other is the\n",
      "ensemble framework that the input signal is segmented into several windows and each window is fed into FCWTA to obtain\n",
      "prediction results. Finally, a soft voting method was implemented to fuse all prediction results.\n",
      "\n",
      "In addition, Lu et al. focused on the visualization of learned representation by a two-layer SAE-based DNN, which provides\n",
      "a novel view to evaluate the DL-based MHMS [81]. In their paper, the discriminative power of learned representation can be\n",
      "improved with the increasing of layers.\n",
      "\n",
      "3.2. RBM and its variants for machine health monitoring\n",
      "\n",
      "In the section, some works focus on developing RBM to learn representation from machinery data. Most of works intro-\n",
      "\n",
      "duced here are based on deep belief networks (DBN) that can pretrain a deep neural network (DNN).\n",
      "\n",
      "In [82], a RBM based method for bearing remaining useful life (RUL) prediction was proposed. Linear regression layer was\n",
      "added at the top of RBM after pretraining to predict the future root mean square (RMS) based on a lagged time series of RMS\n",
      "values. Then, RUL was calculated by using the predicted RMS and the total time of the bearing’s life. In their another work [83],\n",
      "they utilized similar structure: DBN-FNN to predict RUL value directly. Liao et al. proposed a new RBM for representation\n",
      "learning to predict RUL of machines [84]. In their work, a new regularization term modeling the trendability of the hidden\n",
      "nodes was added into the training objective function of RBM. Then, unsupervised self-organizing map algorithm (SOM)\n",
      "was applied to transforming the representation learned by the enhanced RBM to one scale named health value. Finally, the\n",
      "health value was used to predict RUL via a similarity-based life prediction algorithm. In [85], a multi-modal deep support vec-\n",
      "tor classiﬁcation approach was proposed for fault diagnosis of gearboxes. Firstly, three modalities features including time, fre-\n",
      "quency and time–frequency ones were extracted from vibration signals. Then, three Gaussian-Bernoulli deep Boltzmann\n",
      "machines (GDBMS) were applied to addressing the above three modalities, respectively. In each GDBMS, the softmax layer\n",
      "was used at the top. After the pretraining and the ﬁne-tuning processes, the probabilistic outputs of the softmax layers from\n",
      "these three GDBMS were fused by a support vector classiﬁcation (SVC) framework to make ﬁnal prediction. Li et al. applied\n",
      "one GDBMS directly on the concatenation feature consisting of three modalities features including time, frequency and time–\n",
      "frequency ones and stacked one softmax layer on top of GDBMS to recognize fault categories [86]. Li et al. adopted a two-\n",
      "layers DBM to learn deep representations of the statistical parameters of the WPT of raw sensory signal for gearbox fault diag-\n",
      "nosis [87]. In this work focusing on data fusion, two DBMs were applied on acoustic and vibratory signals and random forest\n",
      "was applied to fuse the representations learned by these two DBMs. Shao et al. stacked multiple RBM into DBM model for fault\n",
      "diagnosis, whose input is frequency domain data based on Fast Fourier Transform (FFT) [88]. In [89], Zhang et al. utilized deep\n",
      "belief network for ball screw degradation recognition. The input features into DBN model are the fused frequency spectrum of\n",
      "various time domain signals in different sensors. In [90], Wang et al. proposed to use sliding-window spectrum feature (SWSF)\n",
      "as the input feature into DBN model for hydraulic fault diagnosis. In [91], time-domain and frequency-domain statistical fea-\n",
      "tures were extracted and fed into DBN. Then, the PSO-SVM was applied on DBN outputs for fault diagnosis. In [92], Wang et al.\n",
      "used two RBMs to form a DBM model to predict the material removal rate in polishing. Particle Swarm Optimization (PSO)\n",
      "algorithms were introduced to select hyperparameters such as DBN structure and learning rate. In [93], Chen et al. investi-\n",
      "gated the performances of several DNN-based models including DBM, DBN and SAE on four different preprocessing methods\n",
      "such as raw time domain signal, time domain feature, frequency domain feature and time–frequency domain feature. It is\n",
      "shown that these three DNN models are reliable and effective in fault diagnosis and the raw data-based DNN models perform\n",
      "worse compared to other three preprocessing methods. In [94], Gao et al. combined deep belief network and quantum\n",
      "inspired neural network (QINN) for aircraft fuel system fault diagnosis. The input features into DBN consist of time-\n",
      "domain feature and frequency-domain feature. The outputs of DBN were fed into quantum inspired neural network (QINN),\n",
      "which applies linear superposition of multiple DBNs with quantum intervals. In [95], Oh et al. applied DBN on vibration\n",
      "images to extract features and conducted ﬁnal classiﬁcation. The vibration image were generated from vibration sensor signal\n",
      "and Histogram of Oriented Gradients (HOG) was applied on the vibration image as input features into the following DBN.\n",
      "\n",
      "Making use of DBN-based DNN, Ma et al. presented this framework for degradation assessment under a bearing acceler-\n",
      "ated life test [96]. The statistical feature, root mean square (RMS) ﬁtted by Weibull distribution that can avoid areas of ﬂuc-\n",
      "tuation of the statistical parameter and the frequency domain features were extracted as raw input. Beside the evaluation of\n",
      "the ﬁnal classiﬁcation accuracies, t-SNE algorithm was adopted to visualize the learned representation of DBN and outputs of\n",
      "each layer in DBN. They found the addition of hidden layer can increase the discriminative power in the learned represen-\n",
      "tation. Shao et al. proposed DBN for induction motor fault diagnosis in [97]. As shown in Fig. 7, fast fourier transform was\n",
      "applied on raw time series data and the frequency-domain feature were fed into DBN models. Fu et al. employed deep belief\n",
      "\n",
      "\f",
      "224\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "Input: Vibration signals\n",
      "\n",
      "(Health state & different fault states)\n",
      "\n",
      "Fast Fourier Transform(FFT)\n",
      "\n",
      "Data Preprocessing\n",
      "\n",
      "Training set\n",
      "\n",
      "Testing set\n",
      "\n",
      "Data\n",
      "\n",
      "Label\n",
      "\n",
      "Unsupervised\n",
      "\n",
      "learning\n",
      "\n",
      "Training\n",
      "DBN\n",
      "\n",
      "RBM1\n",
      "\n",
      "RBM2\n",
      "\n",
      "...\n",
      "\n",
      "RBMn-1\n",
      "\n",
      "RBMn\n",
      "\n",
      "Supervised\n",
      "fine-tuning\n",
      "\n",
      "Testing\n",
      "\n",
      "Weights\n",
      "Biases\n",
      "\n",
      "Trained DBN\n",
      "\n",
      "Output:\n",
      "\n",
      "Fault tag &\n",
      "\n",
      "Classification rate\n",
      "\n",
      "Fig. 7. Illustrations of DBN for induction motors fault diagnosis in [97].\n",
      "\n",
      "networks for cutting states monitoring [98]. In the presented work, three different feature sets including raw vibration sig-\n",
      "nal, Mel-frequency cepstrum coefﬁcient (MFCC) and wavelet features were fed into DBN as three corresponding different\n",
      "inputs, which were able to achieve robust comparative performance on the raw vibration signal without too much feature\n",
      "engineering. Tamilselvan et al. proposed a multi-sensory DBN-based health state classiﬁcation model. The model was ver-\n",
      "iﬁed in benchmark classiﬁcation problems and two health diagnosis applications including aircraft engine health diagnosis\n",
      "and electric power transformer health diagnosis [99,100]. Tao et al. proposed a DBN based multisensor information fusion\n",
      "scheme for bearing fault diagnosis [101]. Firstly, 14 time-domain statistical features extracted from three vibration signals\n",
      "acquired by three sensors were concatenated together as an input vector to the DBM model. During pre-training, a prede-\n",
      "ﬁned threshold value was introduced to determine its iteration number. In [102], a feature vector consisting of load and\n",
      "speed measure, time domain features and frequency domain features was fed into DBN-based DNN for gearbox fault diag-\n",
      "nosis. In the work of [103], Gan et al. built a hierarchical diagnosis network for fault pattern recognition of rolling element\n",
      "bearings consisting of two consecutive phases where the four different fault locations (including one health state) were\n",
      "ﬁrstly identiﬁed and then discrete fault severities in each fault condition were classiﬁed. In each phase, the frequency-\n",
      "band energy features generated by WPT were fed into DBN-based DNN for pattern classiﬁcation. In [104], raw vibration sig-\n",
      "nals were pre-processed to generate 2D image based on omnidirectional regeneration (ODR) techniques and then, histogram\n",
      "of original gradients (HOG) descriptor was applied on the generated image and the learned vector was fed into DBN for auto-\n",
      "matic diagnosis of journal bearing rotor systems. Zhang et al. proposed an ensemble of DBNs with multi-objective evolution-\n",
      "ary optimization on decomposition algorithm (MOEA/D) for fault diagnosis with multivariate sensory data [105]. DBNs with\n",
      "different architectures can be regarded as base classiﬁers and MOEA/D was introduced to adjust the ensemble weights to\n",
      "achieve a trade-off between accuracy and diversity. Zhang et al. then extended this above framework for one speciﬁc prog-\n",
      "nostics task: the RUL estimation of the mechanical system [106].\n",
      "\n",
      "3.3. CNN for machine health monitoring\n",
      "\n",
      "In some scenarios, machinery data can be presented in a 2D format such as time–frequency spectrum, while in some sce-\n",
      "narios, they are in a 1D format, i.e., time-series. Therefore, CNN models are able to learn complex and robust representation\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "225\n",
      "\n",
      "via its convolutional layer. Intuitively, ﬁlters in convolutional layers can extract local patterns in raw data and stacking these\n",
      "convolutional layers can further build complex patterns. Liu et al. proposed a dislocated time series convolutional neural net-\n",
      "work (DTS-CNN) for fault diagnosis of electric machine [107]. In their work, a dislocated time series layer was introduced to\n",
      "dislocate the 1D input mechanical signal into an output matrix. The DTS operation is intuitive that several intercepted signals\n",
      "from the original signal were arranged to form a matrix. Then, a conventional deep CNN model was applied. In [108], 2D CNN\n",
      "was introduced for gearbox fault diagnosis. As shown in Fig. 8, wavelet anaylsis was condcuted to transfer raw sensory input\n",
      "into 2D time–frequency images and a deep convolutional neural network was adopted for gearbox fault diagnosis. Janssens\n",
      "et al. utilized a 2D-CNN model for four categories rotating machinery conditions recognition, whose input was DFT results of\n",
      "two accelerometer signals from two sensors that were placed perpendicular to each other. Therefore, the height of input is\n",
      "the number of sensors. The adopted CNN model consisited of one convolutional layer and one fully connected layer. Then,\n",
      "the top softmax layer was adopted for classiﬁcation [109]. Lu et al. rearranged the raw time series data into a 2D map based\n",
      "on slipping insertion whose size is 20 by 20 [110]. In [111], Babu et al. built a 2D deep convolution neural network to predict\n",
      "the RUL of system based on normalized-variate time series from sensor signals, in which one dimension of the 2D input is\n",
      "number of sensors as the setting reported in [109]. In their model, average pooling was adopted instead of max pooling. Since\n",
      "RUL is a continuous value, linear regression was used on the top layer. Ding et al. proposed a deep Convolutional Network\n",
      "(ConvNet) where wavelet packet energy (WPE) image was used as input for spindle bearing fault diagnosis [112]. To fully\n",
      "discover the hierarchical representation, a multiscale layer was added after the last convolutional layer, which concatenates\n",
      "the outputs of the last convolutional layer and the ones of the previous pooling layer. Guo et al. proposed a hierarchical adap-\n",
      "tive deep convolution neural network (ADCNN) [113]. Firstly, the input time series data as a signal-vector was transformed\n",
      "into a 32 \u0002 32 matrix, which follows the typical input format adopted by LeNet [114]. In addition, they designed a hierar-\n",
      "chical framework to recognize fault patterns and fault size. In the fault pattern decision module, the ﬁrst ADCNN was\n",
      "adopted to recognize fault type. In the fault size evaluation layer, based on each fault type, ADCNN with the same structure\n",
      "was used to predict fault size. Here, the classiﬁcation mechanism is still used. The predicted value f is deﬁned as the prob-\n",
      "ability summation of the typical fault sizes as follows:\n",
      "\n",
      "ð19Þ\n",
      "\n",
      "Xc\n",
      "\n",
      "j¼1\n",
      "\n",
      "f ¼\n",
      "\n",
      "ajpj\n",
      "\n",
      "where ½p1; . . . ; pc is produced by the top softmax layer, which denotes the probability score that each sample belongs to each\n",
      "class size and aj is the fault size corresponding to the j-th fault size. Sun et al. adopted dual tree complex wavelet transform\n",
      "(DTCWT) to transform raw time-series signal into 2D map, which can approximate shift-invariance and inhibited frequency\n",
      "aliasing and fed the 2D map into CNN models [115]. In [116], an enhanced CNN was proposed for machinery fault diagnosis.\n",
      "To pre-process vibration data, morlet wavelet was used to decompose the vibration signal and obtain wavelet scaleogram.\n",
      "Then, bilinear interpolation was used to rescale the scaleogram into a grayscale image with a size of 32 \u0002 32. In addition, the\n",
      "adaptation of rectiﬁed linear unit and dropout both boosted the model’s diagnosis performance. Chen et al. adopted a 2D-\n",
      "CNN for gearbox fault diagnosis, in which the input matrix with a size of 16 \u0002 16 for CNN was reshaped by a vector contain-\n",
      "ing 256 statistic features including RMS values, standard deviation, skewness, kurtosis, rotation frequency, and applied load\n",
      "[117]. In addition, 11 different structures of CNN were evaluated empirically in their experiments. Weimer et al. did a com-\n",
      "prehensive study of various design conﬁgurations of deep CNN for visual defect detection [118]. In one speciﬁc application:\n",
      "industrial optical inspection, two directions of model conﬁgurations including depth (addition of conv-layer) and width\n",
      "(increase of number ﬁlters) were investigated. The optimal conﬁguration veriﬁed empirically has been presented in Table 2.\n",
      "In [119], CNN was applied in the ﬁeld of diagnosing the early small faults of front-end controlled wind generator (FSCWG)\n",
      "where the 784 \u0002 784 input matrix consisted of vibration data of generator input shaft (horizontal) and vibration data of gen-\n",
      "erator output shaft (vertical) in time scale. In [120], You et al. used support vector machine as the classiﬁer on the features\n",
      "extracted by CNN for fault diagnosis of rotating machinery. In Lee’s work, they adopted CNN for fault classiﬁcation and diag-\n",
      "nosis in semiconductor manufacturing processes [121]. The 2D input matrix into CNN model was with a processing time axis\n",
      "and a sensor variable axis and the ﬁlter was only slid along the processing time axis. The subsequent pooling operation was\n",
      "conducted on the time axis for each feature map. In [122], Wen et al. ﬁrstly transformed the input raw time-series signal into\n",
      "2D image by sampling segments randomly from the raw signal. They fed the 2D image into a very classical 2D CNN structure:\n",
      "\n",
      "Vibration Signal\n",
      "Normal\n",
      "\n",
      "Fault severity 1\n",
      "\n",
      "Fault severity 2\n",
      "\n",
      "Time-Frequency Spectra\n",
      "\n",
      "Normal\n",
      "\n",
      "Fault severity 1\n",
      "\n",
      "Wavelet\n",
      "Analysis\n",
      "\n",
      "Fault severity 2\n",
      "\n",
      "Deep Convolutional\n",
      "\n",
      "Neural Network\n",
      "\n",
      "Fig. 8. Illustrations of the proposed 2D-CNN for gearbox fault detection in [108].\n",
      "\n",
      "Gearbox Anomaly\n",
      "\n",
      "Detection\n",
      "\n",
      "Fault Severity\n",
      "Classification\n",
      "\n",
      "\f",
      "226\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "Table 2\n",
      "Summary on conﬁgurations of CNN-based MHMS.\n",
      "\n",
      "2D CNN\n",
      "\n",
      "1D CNN\n",
      "\n",
      "Proposed Models\n",
      "\n",
      "Liu’s work [107]\n",
      "Janssens’s work [109]\n",
      "Babu’s work [111]\n",
      "Ding’s work [112]\n",
      "Guo’s work [113]\n",
      "Wang’s work [116]\n",
      "Chen’s work [117]\n",
      "Weimer’s work [118]\n",
      "Dong’s work [119]\n",
      "You’s work [120]\n",
      "\n",
      "Ince’s work [126]\n",
      "Abdeljaber’s work [127]\n",
      "Zhang’s work [130]\n",
      "Sun’s work [132]\n",
      "\n",
      "⁄\n",
      "\n",
      "Conﬁgurations of CNN Structures\n",
      "Input½10 \u0002 512 \u0000 16C½3 \u0002 4 \u0000 16C½3 \u0002 4 \u0000 16P½2 \u0002 2 \u0000 32C½3 \u0002 4 \u0000 32C½3 \u0002 4 \u0000 32P½2 \u0002 2 \u0000 FC½\u0003 \u0000 FC½\u0003\n",
      "Input½5120 \u0002 2 \u0000 32C½64 \u0002 2 \u0000 FC½200\n",
      "Input½27 \u0002 15 \u0000 8C½27 \u0002 4 \u0000 8P½1 \u0002 2 \u0000 14C½1 \u0002 3 \u0000 14P½1 \u0002 2\n",
      "Input½32 \u0002 32 \u0000 20C½7 \u0002 7 \u0000 20P½2 \u0002 2 \u0000 10C½6 \u0002 6 \u0000 10P½2 \u0002 2 \u0000 6P½2 \u0002 2 \u0000 FC½185 \u0000 24\n",
      "Input½32 \u0002 32 \u0000 5C½5 \u0002 5 \u0000 5P½2 \u0002 2 \u0000 10C½5 \u0002 5 \u0000 10P½2 \u0002 2 \u0000 10C½2 \u0002 2 \u0000 10P½2 \u0002 2 \u0000 FC½100 \u0000 FC½50\n",
      "Input½32 \u0002 32 \u0000 64C½3 \u0002 3 \u0000 64P½2 \u0002 2 \u0000 64C½4 \u0002 4 \u0000 64P½2 \u0002 2 \u0000 128C½3 \u0002 3 \u0000 128P½2 \u0002 2 \u0000 FC½512\n",
      "Input½16 \u0002 16 \u0000 8C½5 \u0002 5 \u0000 8P½2 \u0002 2\n",
      "Input½32 \u0002 32 \u0000 64C½3 \u0002 32 \u0000 64P½2 \u0002 2 \u0000 128C½3 \u0002 33 \u0000 128P½2 \u0002 2 \u0000 FC½1024 \u0000 1024\n",
      "Input½784 \u0002 784 \u0000 12C½10 \u0002 10 \u0000 12P½2 \u0002 2 \u0000 24C½10 \u0002 10 \u0000 24P½2 \u0002 2 \u0000 FC½200\n",
      "Input½32 \u0002 32 \u0000 5C½5 \u0002 5 \u0000 5P½2 \u0002 2 \u0000 10C½5 \u0002 5 \u0000 10P½2 \u0002 2 \u0000 10C½2 \u0002 2 \u0000 10P½2 \u0002 2 \u0000 FC½100\n",
      "Input½240 \u0000 60C½9 \u0000 60P½4 \u0000 40C½9 \u0000 40P½4 \u0000 40C½9 \u0000 40P½4 \u0000 FC½20\n",
      "Input½128 \u0000 64C½41 \u0000 64P½2 \u0000 32C½41 \u0000 32P½2 \u0000 FC½10 \u0000 10\n",
      "Input½2048 \u0000 16C½64 \u0000 16P½2 \u0000 4½32C½3 \u0000 32P½2 \u0000 FC½100\n",
      "Input½799 \u0000 100C½200 \u0000 100P½20\n",
      "\n",
      "⁄\n",
      "\n",
      "The symbol Input, C, P and FC denote the raw input, convolutional layer, pooling layer and Fully-connected layer, respectively. Here, \u0003 denotes the\n",
      "\n",
      "undisclosed hyperparameter in the corresponding paper.\n",
      "\n",
      "Lenet-5 CNN. Their model achieved very promising results in three different machinery diagnosis tasks including motor\n",
      "bearing fault diagnosis, self-priming centrifugal pump fault diagnosis and axial piston hydraulic pump fault diagnosis. In\n",
      "[123], various CNN trained on different sensors were fused based on an improved Dempster-Shafer theory to derive ﬁnal\n",
      "prediction. The root mean square maps from Fast Fourier Transformations from sensory data were extracted as the input\n",
      "features for CNN models. In [124], Singh et al. utilized ensemble empirical model decomposition (EEMD) to decompose\n",
      "raw sensory input into intrinsic mode functions (IMF) with selection criteria based on combined model functions (CMF) algo-\n",
      "rithms, which were used as the input features for CNN models. In [125], short-time Fourier transform, wavelet transform and\n",
      "Hilbert-Huang transformation were adopted to generate image inputs into their CNN model.\n",
      "\n",
      "As reviewed in Section 2.3, CNN can also be applied to 1D time series signal and the corresponding operations have been\n",
      "elaborated. In [126], the 1D CNN was successfully developed on raw time series data for motor fault detection, in which fea-\n",
      "ture extraction and classiﬁcation were integrated together. Abdeljaber et al. proposed 1D CNN on normalized vibration sig-\n",
      "nal, which can perform vibration-based damage detection and localization of the structural damage in real-time. The\n",
      "advantage of this approach is its ability to extract optimal damage-sensitive features automatically from the raw accelera-\n",
      "tion signals, which does not need any additional preprocessing or signal processing approaches [127]. Jing et al. investigated\n",
      "performances of 1D CNN on different data types including raw time data, frequency spectrum data, time–frequency data and\n",
      "several hand-crafted features for gearbox fault detection and CNN achieved the best performance with the feature spectrum\n",
      "[128]. In [129], Zhang et al. proposed CNN with Training Interference (TICNN) for bearing fault diagnosis whose input is raw\n",
      "time-series signal. For data augmentation, kernel with changing dropout rate was applied to the input signal and the batch\n",
      "size was set to be the same value as the number of fault types, which can improve the generalization ability of the trained\n",
      "model. Due to these two modiﬁcations, their proposed model was able to achieve high accuracy and stable performance\n",
      "under noisy and varying environment. In [130], Deep Convolutional Neural Networks with Wide First-layer Kernels\n",
      "(WDCNN) was proposed by Zhang et al. The proposed method used raw vibration signals as input (data augmentation\n",
      "was used to generate more inputs), and applied the wide kernels in the ﬁrst convolutional layer for extracting features\n",
      "and suppressing high frequency noise. Small convolutional kernels in the preceding layers were employed for multi-layer\n",
      "nonlinear mapping. An technique named Adaptive Batch Normalization [131] that parameters in batch normalization were\n",
      "adjusted according to testing samples was implemented to improve the domain adaptation ability of the model.\n",
      "\n",
      "Different from these previous works where supervised CNNs were adopted, Sun et al. proposed a convolutional discrim-\n",
      "inative feature learning model to detect induction motor fault diagnosis [132]. As shown in Fig. 9, a feed-forward convolu-\n",
      "tional pooling architecture was proposed, in which local ﬁlters were pre-learned by back-propagation-based neural network\n",
      "(BPNN). Then, the learned representation was fed into SVM for fault conditions classiﬁcation. Since local ﬁlters are learned by\n",
      "BPNN, the following convolutional pooling architecture can extract discriminative and invariant features from the raw vibra-\n",
      "tion data quickly. The input data is 1D vibration signal so that their work also belongs to 1D CNN. In [133], Cabrera et al.\n",
      "adopted convolutional autoencder (CAE) to initialize their supervised CNN model parameters. In CAE, the encoder consists\n",
      "of convolution and max-pooling while the decoder consists of un-pooling as horizontal and vertical replication of activation\n",
      "value and convolution. The training objective of CAE was deﬁned as euclidean distance. In [134], Shao et al. incorporated\n",
      "CNN into DBN by employing convolutional connections in the generative Markov random ﬁle structure. In addition, Gaussian\n",
      "visible units were introduced to construct this model. The input into the model was the compressed data learned by auto-\n",
      "ecnoder as the hidden representations. The softmax classiﬁer was used for bearing fault diagnosis.\n",
      "\n",
      "In [135], Zhao et al. developed a variant of deep residual networks named as deep residual networks with dynamically\n",
      "weighted wavelet coefﬁcients (DRN + DWWC). The inputs into the model is a series of wavelet packet coefﬁcients on various\n",
      "frequency bands. The DRN consists of several residual building block as a stack of several convolutional layers, batch normal-\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "227\n",
      "\n",
      "Induction Motor\n",
      "\n",
      "Sensor\n",
      "\n",
      "Convolution\n",
      "\n",
      "Pooling\n",
      "\n",
      "SVM clasifier\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "K\n",
      "\n",
      "Input Signal\n",
      "\n",
      "Input Layer\n",
      "Windows Size\n",
      "\n",
      "Filter Weights\n",
      "\n",
      "Hidden Layer\n",
      "Filter Number K\n",
      "\n",
      "...\n",
      "\n",
      "Fig. 9. Illustrations of the proposed unsupervised CNN for induction motor fault diagnosis in [132].\n",
      "\n",
      "izations (BNs), relu activation function and one identity shortcut. In traditional DRN, dynamic weighting layers were\n",
      "designed to apply dynamic weights to the input feature map and elements in each row in feature map shared the same\n",
      "weight. The utilization of dynamic weighting layers focused on emphasizing different contributions of wavelet packet coef-\n",
      "ﬁcients on different frequency bands. In [136], Pan et al. proposed a novel CNN model named LiftingNet for fault classiﬁca-\n",
      "tion, which was based on CNN and Second Generation Wavelet Transform (SGWT). The basic module in LiftingNet consists of\n",
      "split layer, predict layer and update layer. Split layer divided the input sequence into an even series and an odd series. Then,\n",
      "predict and update layers applied circular convolution operation to learn representations of input on different scales via\n",
      "using different kernel sizes. After stacking of the above modules, max-pooling layer and fully-connected layer were applied\n",
      "to learning the ﬁnal representation. It has been veriﬁed that LiftingNet can learn transient, high-frequency component via\n",
      "predict layer and encode gradual, low-frequency component via update layer. In addition, the large-size kernels and nonlin-\n",
      "ear functions were able to ﬁlter noise.\n",
      "\n",
      "To present an overview about all these above CNN models that have been successfully applied in the area of MHMS, their\n",
      "architectures have been summarized in Table 2. To explain the used abbreviation, the structure of CNN applied in Weimer’s\n",
      "work [118] is denoted as Input½32 \u0002 32 \u0000 64C½3 \u0002 32 \u0000 64P½2 \u0002 2 \u0000 128C½3 \u0002 33 \u0000 128P½2 \u0002 2 \u0000 FC½1024 \u0000 10242.\n",
      "It\n",
      "means the input 2D data is 32 \u0002 32 and the CNN ﬁrstly applies 2 convolutional layers with the same design that the number\n",
      "of ﬁlters is 64 and the ﬁlter size is 3 \u0002 3, then stackes one max-pooling layer whose pooling size is 2 \u0002 2, then applies 3 con-\n",
      "volutional layers with the same design that the number of ﬁlters is 128 and the ﬁler size is 3 \u0002 3, then applies a pooling layer\n",
      "whose pooling size is 2 \u0002 2, and ﬁnally adopts two fully-connected layers whose hidden neuron numbers are both 1024. It\n",
      "should be noted that the size of output layer is not given here, considering it is task-speciﬁc and usually set to be the number\n",
      "of categories.\n",
      "\n",
      "3.4. RNN for machine health monitoring\n",
      "\n",
      "The majority of machinery data belong to sensor data, which are in nature time series. RNN models including LSTM and\n",
      "GRU have emerged as one kind of popular architectures to handle sequential data with its ability to encode temporal infor-\n",
      "mation. These advanced RNN models have been proposed to relief the difﬁculty of training behind vanilla RNN for machine\n",
      "health monitoring recently. In [137], Yuan et al. investigated three RNN models including vanilla RNN, LSTM and GRU models\n",
      "for fault diagnosis and prognostics of aero engine. They found these advanced RNN models of LSTM and GRU outperformed\n",
      "vanilla RNN. Another interesting observation was the ensemble model of the above three RNN variants did not boost the\n",
      "performance of LSTM. Zhao et al. presented an empirical evaluation of LSTMs-based machine health monitoring system in\n",
      "the tool wear test [138]. The applied LSTM model encoded the raw sensory data into vectors and predicted the corresponding\n",
      "tool wear. Zhao et al. further designed a more complex deep learning model combining CNN and LSTM named Convolutional\n",
      "Bi-directional Long Short-Term Memory Networks (CBLSTM) [139]. As shown in Fig. 10, CNN was used to extract robust local\n",
      "features from the sequential input, and then bi-directional LSTM was adopted to encode temporal information on the\n",
      "\n",
      "\f",
      "228\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "Convolutional ReLu\n",
      "\n",
      "Pooling\n",
      "\n",
      "Output \n",
      "\n",
      "Representation\n",
      "\n",
      "Dropout\n",
      "\n",
      "Dropout\n",
      "\n",
      "Raw Sensory Signal\n",
      "\n",
      "Local Feature Extractor\n",
      "\n",
      "Convolutional Neural Network\n",
      "\n",
      "Temporal Encoder\n",
      "\n",
      "Deep Bi-directional LSTM\n",
      "\n",
      "Two Fully-connected Layers \n",
      "and Linear Regression Layer\n",
      "\n",
      "Fig. 10. Illustrations of the proposed Convolutional Bi-directional Long Short-Term Memory Networks in [139].\n",
      "\n",
      "sequential output of CNN. Stacked fully-connected layers and linear regression layer were ﬁnally added to predict the target\n",
      "value. In tool wear test, the proposed model was able to outperform several state-of-the-art baseline methods including con-\n",
      "ventional LSTM models. Different from the previous automatic feature learning models, Zhao et al. proposed a hybrid\n",
      "approach that combines handcrafted feature design with automatic feature learning for machine health monitoring [140].\n",
      "As illustrated in Fig. 11, features from windows of input time series were extracted and fed into an enhanced bi-\n",
      "directional GRU network. The enhanced bi-directional GRU network consists of two modules including bi-directional GRU\n",
      "and weighted feature averaging. Their proposed model in three machine health monitoring tasks: tool wear prediction, gear-\n",
      "box fault diagnosis and incipient bearing fault detection has shown the effectiveness and generalization of the proposed\n",
      "hybrid approach combing handcrafted feature design and automatic feature learning. In [141], Malhotra proposed a very\n",
      "interesting structure for RUL prediction. They designed a LSTM-based encoder-decoder structure, which LSTM-based enco-\n",
      "der ﬁrstly transformed a multivariate input sequence to a ﬁxed-length vector and then, LSTM decoder used vectors to pro-\n",
      "duce the target sequence. When it comes to RUL prediction, their assumptions lies that the model can be ﬁrstly trained in\n",
      "raw signal corresponding to normal behavior in an unsupervised way. Then, the reconstruction error can be used to compute\n",
      "\n",
      "Fig. 11. Illustrations of the proposed Local Feature-based Gated Recurrent Unit Networks in [140].\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "229\n",
      "\n",
      "health index (HI), which is then used for RUL estimation. It is intuitive that the large reconstruction error corresponds to an\n",
      "unhealthy machine condition.\n",
      "\n",
      "3.5. Fault feature extraction for DL-based MHMS\n",
      "\n",
      "In the above sessions, the applications of various deep learning methods in machine health monitoring systems have been\n",
      "reviewed. It can be found that feature extraction or feature preprocessing are required in certain works. Although deep learn-\n",
      "ing can learn representations from scratch, the noisy sensory data acquired from machines, the complexity in the machinery\n",
      "operating systems and the insufﬁcient data samples all make it is necessary to apply fault feature extraction before raw data\n",
      "are fed into DL models. In addition, rare and inconsistent sensory signal, i.e., outliers, may have some unpredictable inﬂuence\n",
      "on the nonlinear and real mechanical system [142–144]. The appropriate fault feature extraction can alleviate this negative\n",
      "effect of outliers. Therefore, a brief summary of these feature extraction techniques adopted in the above works is given fol-\n",
      "lowing several aspects:\n",
      "\n",
      "3.5.1. Time domain\n",
      "\n",
      "Raw sensory data is in nature time-series signal. Therefore, some statistical time-domain features could be extracted as\n",
      "discriminative features fed into DL systems. Mean, root mean square (RMS), standard deviation and variances were used a\n",
      "lot. Due to the stationary signal, skewness and kurtosis were also extracted as input features.\n",
      "\n",
      "3.5.2. Frequency domain\n",
      "\n",
      "Due to the existence of periodical impulse in several machinery faults, their dominant frequency components are infor-\n",
      "mative and discriminative features. FFT (Fast Fourier Transform) can be applied to transforming the time-domain vibration\n",
      "signals into frequency-domain ones. Mean frequency, root variance frequency, spectral skewness and spectral kurtosis as\n",
      "ﬁrst-order, second-order, third-order and fourth-order moments of the Fourier spectrum can be calculated as features. It\n",
      "should be noted that frequency domain representations are commonly used for stationary signal.\n",
      "\n",
      "3.5.3. Time-frequency domain\n",
      "\n",
      "Time-frequency domain features are useful for non-stationary signal. Short-time Fourier Transform, wavlet transform/\n",
      "decomposition and empricial model decomposition (EMD) are widely used to convert the one-dimensional signal into\n",
      "two-dimensional signal of time and frequency.\n",
      "\n",
      "In addition, we provided implementations of several feature extraction methods in these three above domains,1 which\n",
      "\n",
      "will be illustrated in the following section with more details.\n",
      "\n",
      "4. Experimental evaluations of various DL techniques\n",
      "\n",
      "In this section, a systematic evaluation of these above discussed deep learning models on machine health monitoring\n",
      "\n",
      "tasks is provided. In particular, the tool wear sensing task has been introduced.\n",
      "\n",
      "4.1. Implementation details\n",
      "\n",
      "Dataset description: dataset were sampled from a high speed CNC machine during dry milling operations and its sche-\n",
      "matic diagram of experimental platform is shown in 12. The detailed experimental settings can be found in [145], where\n",
      "seven sensors including force and vibration ones in three directions and AE-RMS have been placed. The ground-truth value\n",
      "were obtained by using a LEICA MZ12 microscope to measure each individual ﬂute after ﬁnishing each surface, i.e., each cut\n",
      "number. Machine learning models are adopted to predict the actual ﬂank wear from the sensory data. Three individual cutter\n",
      "records denoted as c1, c4 and c6 are available and each record contains 315 data samples.\n",
      "\n",
      "In our experiments, c4 is used as testing data while the other records c1 and c6 are used as training data. Considering the\n",
      "high dimensionality of the raw time series signal, feature extractions are applied ﬁrstly. Seven kinds of features including\n",
      "time domain, frequency domain and time–frequency domain are designed, which could be found in Table 3. Here, the wave-\n",
      "let energy feature is the energy of a 8-level wavelet packet decomposition using db1, which corresponds to the wavelet coef-\n",
      "ﬁcient with higher energy that is related to the characteristic frequency of the machine. Considering seven sensors were\n",
      "deployed, the dimensionality of the hand-crafted feature vector is 70. For LSTM and CNN, the input data is tensor so that\n",
      "the data is sliced into 20 windows and then fed into feature extraction, respectively. For other models such as SVM, the input\n",
      "data is vector so that the whole time series is fed into feature extraction. Therefore, we have two kinds of extracted features,\n",
      "one is in shape of 20*70 and the other is in shape of 70.\n",
      "\n",
      "Compared Approaches: These following methods are compared:\n",
      "\n",
      "1 https://github.com/ClockworkBunny/MHMS_DEEPLEARNING/blob/master/code/feature_extract.py.\n",
      "\n",
      "\f",
      "230\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "* Linear SVR: Linear Support Vector Regression, whose input features are extracted features of the whole time series. The\n",
      "\n",
      "regularization term is set to 1.\n",
      "\n",
      "* RBF SVR: Support Vector Regression with RBF kernel, whose input features are extracted features of the whole time series.\n",
      "\n",
      "The regularization term is set to 1.\n",
      "\n",
      "* Random Forest: Random Forest Regressor whose input features are extracted features of the whole time series. The num-\n",
      "\n",
      "ber of estimator is set to 50 and the max depth of decision tree is set to 2.\n",
      "\n",
      "* Neural Network: neural network whose input features are extracted features of the whole time series. The neural network\n",
      "contains two hidden layers, whose sizes are 70 and 140, respectively. To prevent overﬁting, the dropout layer with a\n",
      "masking probability of 0.2 is applied on the last layer.\n",
      "\n",
      "CNC Milling Machine\n",
      "\n",
      "Cutting Tooth\n",
      "\n",
      "z\n",
      "\n",
      "Workpiece\n",
      "Machining \n",
      "\n",
      "Table\n",
      "\n",
      "Tool State\n",
      "\n",
      "y\n",
      "x\n",
      "Accelerometer\n",
      "Force Sensor\n",
      "\n",
      "Offline Measurement\n",
      "\n",
      "Microscope\n",
      "LEICA MZ12\n",
      "\n",
      "Tool Wear \n",
      "\n",
      "Width\n",
      "\n",
      "Online Measurement\n",
      "\n",
      "Charge \n",
      "Amplifier\n",
      "\n",
      "Acquisition \n",
      "\n",
      "Data \n",
      "\n",
      "Card\n",
      "\n",
      "PC\n",
      "\n",
      "Force, Vibration \n",
      "Measurements\n",
      "\n",
      "Fig. 12. Schematic of the experimental setup for tool wear prediction [23].\n",
      "\n",
      "Table 3\n",
      "List of Extracted Features.\n",
      "\n",
      "Domain\n",
      "\n",
      "Statistical\n",
      "\n",
      "Features\n",
      "\n",
      "RMS\n",
      "\n",
      "Variance\n",
      "Maximum\n",
      "Skewness\n",
      "Kurtosis\n",
      "Peak-to-Peak\n",
      "\n",
      "Frequency\n",
      "\n",
      "Spectral Skewness\n",
      "\n",
      "Spectral Kurtosis\n",
      "\n",
      "Time–Frequency\n",
      "\n",
      "Wavelet Energy\n",
      "\n",
      "n\n",
      "\n",
      "n\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "q\n",
      "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n",
      "Pn\n",
      "P\n",
      "i¼1z2\n",
      "i\n",
      "i¼1ðzi \u0000 \u0016zÞ2\n",
      "\n",
      "Expression\n",
      "zrms ¼\n",
      "zvar ¼ 1\n",
      "zmax ¼ maxðzÞ\n",
      "zskew ¼ E½ðz\u0000l\n",
      "r Þ3\n",
      "zkurt ¼ E½ðz\u0000l\n",
      "r Þ4\n",
      "zp\u0000p ¼ maxðzÞ \u0000 minðzÞ\n",
      "P\n",
      "P\n",
      "r Þ3\n",
      "i¼1kðf i\u0000\u0016f\n",
      "f skew ¼\n",
      "Sðf iÞ\n",
      "r Þ4\n",
      "i¼1kðf i\u0000\u0016f\n",
      "f kurt ¼\n",
      "Sðf iÞ\n",
      "PN\n",
      "EWT ¼\n",
      "/ðiÞ=N\n",
      "i¼1wt2\n",
      "\n",
      "Table 4\n",
      "The Results of all the approaches in tool wear prediction under\n",
      "the two criteria including MAE and MSE.\n",
      "\n",
      "Method\n",
      "\n",
      "MAE\n",
      "\n",
      "MSE\n",
      "\n",
      "Linear SVR\n",
      "RBF SVR\n",
      "Random Forest\n",
      "Neural Network\n",
      "Auto-encoder\n",
      "Denoising Auto-encoder\n",
      "DBN\n",
      "LSTM\n",
      "Bi-directional LSTM\n",
      "CNN\n",
      "\n",
      "13:7\n",
      "17:6\n",
      "14:4 \u0006 0:2\n",
      "11:5 \u0006 1:2\n",
      "11:2 \u0006 1:9\n",
      "9:3 \u0006 1:8\n",
      "11:7 \u0006 1:3\n",
      "11:0 \u0006 2:5\n",
      "10:6 \u0006 1:9\n",
      "11:0 \u0006 1:3\n",
      "\n",
      "248:9\n",
      "402:9\n",
      "289 \u0006 5:9\n",
      "191:6 \u0006 26:8\n",
      "185 \u0006 41:9\n",
      "143:5 \u0006 40:4\n",
      "194:6 \u0006 34:2\n",
      "193:9 \u0006 65:3\n",
      "191:5 \u0006 56:1\n",
      "197:2 \u0006 30:9\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "231\n",
      "\n",
      "* Auto-encoder: auto-encoder whose input features are extracted features of the whole time series. The loss function of pre-\n",
      "training is mean-squared error. The sizes of hidden layers in pre-training are 100 and 140, respectively. In supervised\n",
      "training, a layer with a size of 900 is added following these two hidden layers.\n",
      "\n",
      "* Denoising-Auto-encoder: denoising auto-encoder whose input features are extracted features of the whole time series.\n",
      "Compared to normal auto-encoder, a dropout noise is added on the input features with a masking probability of 0.01.\n",
      "\n",
      "(a)\n",
      "\n",
      "(c)\n",
      "\n",
      "(e)\n",
      "\n",
      "(g)\n",
      "\n",
      "(i)\n",
      "\n",
      "(b)\n",
      "\n",
      "(d)\n",
      "\n",
      "(f)\n",
      "\n",
      "(h)\n",
      "\n",
      "(j)\n",
      "\n",
      "Fig. 13. Regression Result of All Compared Methods. (a) Linear SVR, (b) RBF SVR, (c) Random Forest, (d) Neural Network, (e) Autoencoder, (f) Denoising\n",
      "Autoencoder, (g) DBN, (h) LSTM, (i) Bi-LSTM, (j) CNN.\n",
      "\n",
      "\f",
      "232\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "* DBN: deep belief network whose input features are extracted features of the whole time series. The hidden layer sizes are\n",
      "\n",
      "100 and 140, respectively.\n",
      "\n",
      "* CNN: Convolutional Neural Network whose input features are sequential features. Two 1D convolutional layers with win-\n",
      "dows sizes of 3 are adopted. And their hidden layer sizes are set to 100 and 140. Then, one max-pooling layer is added,\n",
      "which is followed by a fully-connected layer with a size of 900 and a dropout layer whose dropout probability is 0.2.\n",
      "\n",
      "* LSTM: Long-short Term Memory Network whose input features are sequential features. We stacked two recurrent layers\n",
      "whose hidden layer sizes are set to 100 and 140. A fully-connected layer with a size of 900 and a dropout layer with a\n",
      "masking probability of 0.2 are added.\n",
      "\n",
      "* Bi-directional LSTM: Bi-directional Long-short Term Memory Network whose input features are sequential features. Com-\n",
      "pared to LSTM, the data are fed into bi-directional LSTM in two directions: from beginning to end and from end to\n",
      "beginning.\n",
      "\n",
      "In addition, the dataset and code have been published2. Due to the privacy issue and potential copyright concern, we only\n",
      "provide the extracted features for these data instead of raw time-series. Since almost all deep learning models require random\n",
      "parameter initialization, all the comparative models were run ﬁve times. Here, we adopt two measures including mean absolute\n",
      "error (MAE) and mean squared error (MSE).\n",
      "\n",
      "MAE ¼ 1\n",
      "n\n",
      "\n",
      "RMSE ¼\n",
      "\n",
      "Xn\n",
      "j~yi \u0000 yij\n",
      "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n",
      "vuut\n",
      "Xn\n",
      "ð~yi \u0000 yiÞ2\n",
      "\n",
      "i¼1\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "i¼1\n",
      "\n",
      "ð20Þ\n",
      "\n",
      "ð21Þ\n",
      "\n",
      "where yi and ~yi are true and predicted too wear depth.\n",
      "\n",
      "4.2. Experimental results\n",
      "\n",
      "The performances of all compared methods are shown in Table 4. In addition, the regression results are all visualized in\n",
      "Fig. 13. It should be noted that the hyper-parameters of all models are set to be default values without ﬁne-tuning. In this\n",
      "section, we are attempting to shed some lights over the application of deep learning models on machine health monitoring\n",
      "tasks. It should be noted that, due to the default setting of hyper-parameter selection and the small data size, the improve-\n",
      "ment of deep learning models compared to traditional methods is not so huge. However, as shown in Table 4, it still could be\n",
      "found that deep learning models perform better than these conventional machine learning models. In addition, due to pre-\n",
      "training, auto-encoder is able to achieve better performance than neural network. However, DBN performed slightly worse\n",
      "than neural network, which may be explained by the fact that DBN is hard to train due to sampling required at each layer.\n",
      "The introduction of dropout noise further improve the performances of denoising-autoencoders. Although CNN, LSTM and\n",
      "Bi-LSTM all perform slightly worse than denoising autoencoder, hyperparameter selection may boost their performances\n",
      "considering the complexity of these models. But bi-directional LSTM performs better than LSTM. It means that the bi-\n",
      "directional processing of time-series signal is suitable for machine health monitoring.\n",
      "\n",
      "* Compared to conventional machine learning models, deep learning models are able to achieve superior performances in\n",
      "\n",
      "the ﬁeld of machine health monitoring.\n",
      "\n",
      "* Pre-training in autoencoder can boost the performance of machine health monitoring.\n",
      "* Denoising technique is beneﬁcial for machine health monitoring.\n",
      "* CNN, LSTM and their variants can handle machine health monitoring. However, due to the model complexity, hyperpa-\n",
      "\n",
      "rameter selection is required to achieve state-of-the-art performances.\n",
      "\n",
      "5. Summary and future directions\n",
      "\n",
      "In this paper, we have provided a systematic overview of the state-of-the-art DL-based MHMS. Deep learning, as a sub-\n",
      "ﬁeld of machine learning, is serving as a bridge between big machinery data and data-driven MHMS. Therefore, within the\n",
      "past ﬁve years, they have been applied in various machine health monitoring tasks. These proposed DL-based MHMS are\n",
      "summarized according to four categories of DL architecture as: Auto-encoder models, Restricted Boltzmann Machines mod-\n",
      "els, Convolutional Neural Networks and Recurrent Neural Networks. Since the momentum of the research of DL-based\n",
      "MHMS is growing fast, we hope the messages about the capabilities of these DL techniques, especially representation learn-\n",
      "ing for complex machinery data and target prediction for various machine health monitoring tasks, can be conveyed to read-\n",
      "ers. Through these previous works, it can be found that DL-based MHMS do not require extensive human labor and expert\n",
      "knowledge, i.e., the end-to-end structure is able to map raw machinery data to targets. Therefore, the application of deep\n",
      "\n",
      "2 Please ﬁnd code and data inhttps://github.com/ClockworkBunny/MHMS_DEEPLEARNING.\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "233\n",
      "\n",
      "learning models are not restricted to speciﬁc kinds of machines, which can be a general solution to address the machine\n",
      "health monitoring problems. Besides, some research trends and potential future research directions are given as follows:\n",
      "\n",
      "* Open-source Large Dataset: Due to the huge model complexity behind DL methods, the performance of DL-based MHMS\n",
      "heavily depends on the scale and quality of datasets. On the other hand, the depth of DL model is limited by the scale of\n",
      "datasets. As a result, the benchmark CNN model for image recognition has 152 layers, which can be supported by the\n",
      "large dataset ImageNet containing over ten million annotated images [146,147]. In contrast, the proposed DL models\n",
      "for MHMS may stack up to 5 hidden layers. And the model trained in such kind of large datasets can be the model ini-\n",
      "tialization for the following speciﬁc task/dataset. Therefore, it is meaningful to design and publish large-scale machinery\n",
      "datasets.\n",
      "\n",
      "* Utilization of Domain Knowledge: deep learning is not a skeleton key to all machine health monitoring problems. Domain\n",
      "knowledge can contribute to the success of applying DL models on machine health monitoring. For example, extracting\n",
      "discriminative features can reduce the size of the followed DL models and appropriate task-speciﬁc regularization term\n",
      "can boost the ﬁnal performance [84].\n",
      "\n",
      "* Model and Data Visualization: deep learning techniques, especially deep neural networks, have been regarded as black\n",
      "boxes models, i.e., their inner computation mechanisms are unexplainable. Visualization of the learned representation\n",
      "and the applied model can offer some insights into these DL models, and then these insights achieved by this kind of\n",
      "interaction can facilitate the building and conﬁguration of DL models for complex machine health monitoring problems.\n",
      "Some visualization techniques have been proposed including t-SNE model for high dimensional data visualization [148]\n",
      "and visualization of the activations produced by each layer and features at each layer of a DNN via regularized optimiza-\n",
      "tion [149].\n",
      "\n",
      "* Transferred Deep Learning: Transfer learning tries to apply knowledge learned in one domain to a different but related\n",
      "domain [25]. This research direction is meaningful in machine health monitoring, since some machine health monitoring\n",
      "problems have sufﬁcient training data while other areas lack training data. The machine learning models including DL\n",
      "models trained in one domain can be transferred to the other domain. Some previous works focusing on transferred fea-\n",
      "ture extraction/dimensionality reduction have been done [150,151]. In [152], a Maximum Mean Discrepancy (MMD)\n",
      "measure evaluating the discrepancy between source and target domains was added into the target function of deep neu-\n",
      "ral networks.\n",
      "\n",
      "* Imbalanced Class: The class distribution of machinery data in real life normally follows a highly-skewed one, in which\n",
      "most data samples belong to few categories. For example, the number of fault data is much less than the one of health\n",
      "data in fault diagnosis. Some enhanced machine learning models including SVM and ELM have been proposed to address\n",
      "this imbalanced issue in machine health monitoring [153,154]. Recently, some interesting methods investigating the\n",
      "application of deep learning in imbalanced class problems have been developed, including CNN models with class resam-\n",
      "pling or cost-sensitive training [155] and the integration of boot strapping methods and CNN model [156].\n",
      "\n",
      "It is believed that deep learning will have a more and more prospective future impacting machine health monitoring,\n",
      "\n",
      "especially in the age of big machinery data.\n",
      "\n",
      "Acknowledgment\n",
      "\n",
      "This work has been supported in part by the National Natural Science Foundation of China (51575102).\n",
      "\n",
      "References\n",
      "\n",
      "[1] S. Yin, X. Li, H. Gao, O. Kaynak, Data-based techniques focused on modern industry: an overview, IEEE Trans. Industr. Electron. 62 (1) (2015) 657–667,\n",
      "\n",
      "ISSN 0278-0046.\n",
      "\n",
      "[2] S. Jeschke, C. Brecher, H. Song, D.B. Rawat, Industrial Internet of Things, Springer, 2017.\n",
      "[3] D. Lund, C. MacGillivray, V. Turner, M. Morales, Worldwide and regional internet of things (iot) 2014–2020 forecast: A virtuous circle of proven value\n",
      "\n",
      "and demand, International Data Corporation (IDC), Tech. Rep.\n",
      "\n",
      "[4] Y. Li, T. Kurfess, S. Liang, Stochastic prognostics for rolling element bearings, Mech. Syst. Signal Process. 14 (5) (2000) 747–762.\n",
      "[5] C.H. Oppenheimer, K.A. Loparo, Physically based diagnosis and prognosis of cracked rotor shafts, AeroSense 2002, International Society for Optics and\n",
      "\n",
      "Photonics, 2002, pp. 122–132.\n",
      "\n",
      "[6] M. Yu, D. Wang, M. Luo, Model-based prognosis for hybrid systems with mode-dependent degradation behaviors, IEEE Trans. Industr. Electron. 61 (1)\n",
      "\n",
      "(2014) 546–554.\n",
      "\n",
      "[7] A.K. Jardine, D. Lin, D. Banjevic, A review on machinery diagnostics and prognostics implementing condition-based maintenance, Mech. Syst. Signal\n",
      "\n",
      "Processing 20 (7) (2006) 1483–1510.\n",
      "\n",
      "[8] V. Stojanovic, N. Nedic, D. Prsic, L. Dubonjic, V. Djordjevic, Application of cuckoo search algorithm to constrained control problem of a parallel robot\n",
      "\n",
      "platform, Int. J. Adv. Manuf. Technol. 87 (9–12) (2016) 2497–2507.\n",
      "\n",
      "[9] D. Pršic´, N. Nedic´, V. Stojanovic´, A nature inspired optimal control of pneumatic-driven parallel robot platform, Proc. Inst. Mech. Eng., Part C: J. Mech.\n",
      "\n",
      "Eng. Sci. 231 (1) (2017) 59–71.\n",
      "\n",
      "[10] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: towards real-time object detection with region proposal networks, in: Advances in Neural Information\n",
      "\n",
      "Processing Systems, 91–99, 2015.\n",
      "\n",
      "[11] R. Collobert, J. Weston, A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning, in: Proceedings of the\n",
      "\n",
      "25th International Conference on Machine learning, ACM, 160–167, 2008.\n",
      "\n",
      "[12] G. Hinton, L. Deng, D. Yu, G.E. Dahl, A.-R. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T.N. Sainath, et al, Deep neural networks for acoustic\n",
      "\n",
      "modeling in speech recognition: the shared views of four research groups, IEEE Signal Process. Mag. 29 (6) (2012) 82–97.\n",
      "\n",
      "\f",
      "234\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "[13] M.K. Leung, H.Y. Xiong, L.J. Lee, B.J. Frey, Deep learning of the tissue-regulated splicing code, Bioinformatics 30 (12) (2014) i121–i129.\n",
      "[14] J. Schmidhuber, Deep Learning in Neural Networks: An Overview, Neural Networks 61 (2015) 85–117, doi: 10.1016/j.neunet.2014.09.003, published\n",
      "\n",
      "online 2014; based on TR arXiv:1404.7828 [cs.NE].\n",
      "\n",
      "[15] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436–444.\n",
      "[16] R. Raina, A. Madhavan, A.Y. Ng, Large-scale deep unsupervised learning using graphics processors, in: Proceedings of the 26th Annual International\n",
      "\n",
      "Conference on Machine Learning, ACM, 873–880, 2009.\n",
      "\n",
      "[17] G.E. Hinton, Learning multiple layers of representation, Trends Cognitive Sci. 11 (10) (2007) 428–434.\n",
      "[18] A. Widodo, B.-S. Yang, Support vector machine in machine condition monitoring and fault diagnosis, Mech. Syst. Signal Processing 21 (6) (2007)\n",
      "\n",
      "2560–2574.\n",
      "\n",
      "[19] J. Yan, J. Lee, Degradation assessment and fault modes classiﬁcation using logistic regression, J. Manuf. Sci. Eng. 127 (4) (2005) 912–914.\n",
      "[20] V. Muralidharan, V. Sugumaran, A comparative study of Naïve Bayes classiﬁer and Bayes net classiﬁer for fault diagnosis of monoblock centrifugal\n",
      "\n",
      "pump using wavelet analysis, Appl. Soft Comput. 12 (8) (2012) 2023–2029.\n",
      "\n",
      "[21] Y. Bengio, A. Courville, P. Vincent, Representation learning: Aa review and new perspectives, IEEE Trans. Pattern Anal. Mach. Intell. 35 (8) (2013)\n",
      "\n",
      "1798–1828.\n",
      "\n",
      "[22] A. Malhi, R.X. Gao, PCA-based feature selection scheme for machine defect classiﬁcation, IEEE Trans. Instrum. Meas. 53 (6) (2004) 1517–1525.\n",
      "[23] J. Wang, J. Xie, R. Zhao, L. Zhang, L. Duan, Multisensory fusion based virtual tool wear sensing for ubiquitous manufacturing, Robotics Computer-\n",
      "\n",
      "Integrated Manuf. 45 (2017) 47–58.\n",
      "\n",
      "[24] J. Wang, J. Xie, R. Zhao, K. Mao, L. Zhang, A new probabilistic kernel factor analysis for multisensory data fusion: application to tool condition\n",
      "\n",
      "monitoring, IEEE Trans. Instrum. Meas. 65 (11) (2016) 2527–2537, ISSN 0018-9456.\n",
      "\n",
      "[25] S.J. Pan, Q. Yang, A survey on transfer learning, IEEE Trans. Knowl. Data Eng. 22 (10) (2010) 1345–1359.\n",
      "[26] P. Vincent, H. Larochelle, Y. Bengio, P.-A. Manzagol, Extracting and composing robust features with denoising autoencoders, in: Proceedings of the\n",
      "\n",
      "25th International Conference on Machine Learning, ACM, 1096–1103, 2008.\n",
      "\n",
      "[27] G.E. Hinton, S. Osindero, Y.-W. Teh, A fast learning algorithm for deep belief nets, Neural Comput. 18 (7) (2006) 1527–1554.\n",
      "[28] R. Salakhutdinov, G.E. Hinton, Deep boltzmann machines., in: AISTATS, vol. 1, 3, 2009.\n",
      "[29] P. Sermanet, S. Chintala, Y. LeCun, Convolutional neural networks applied to house numbers digit classiﬁcation, in: Pattern Recognition (ICPR), 2012\n",
      "\n",
      "21st International Conference on, IEEE, 3288–3291, 2012.\n",
      "\n",
      "[30] K.-I. Funahashi, Y. Nakamura, Approximation of dynamical systems by continuous time recurrent neural networks, Neural Networks 6 (6) (1993)\n",
      "\n",
      "801–806.\n",
      "\n",
      "[31] L. Deng, D. Yu, Deep learning: methods and applications, Found. Trends Signal Process. 7 (2014) 197–387.\n",
      "[32] I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016.\n",
      "[33] A. Ng, Sparse autoencoder, CS294A Lecture Notes 72 (2011) 1–19.\n",
      "[34] G. Hinton, A practical guide to training restricted Boltzmann machines, Momentum 9 (1) (2010) 926.\n",
      "[35] A.-R. Mohamed, G.E. Dahl, G. Hinton, Acoustic modeling using deep belief networks, IEEE Trans. Audio, Speech, Language Processing 20 (1) (2012) 14–\n",
      "\n",
      "22.\n",
      "\n",
      "[36] B.B. Le Cun, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, Handwritten digit recognition with a back-propagation network, in:\n",
      "\n",
      "Advances in Neural Information Processing Systems, Citeseer, 1990.\n",
      "\n",
      "[37] K. Jarrett, K. Kavukcuoglu, Y. Lecun, et al., What is the best multi-stage architecture for object recognition?, in: 2009 IEEE 12th International\n",
      "\n",
      "Conference on Computer Vision, IEEE, 2146–2153, 2009.\n",
      "\n",
      "[38] A. Krizhevsky, I. Sutskever, G.E. Hinton, Imagenet classiﬁcation with deep convolutional neural networks, in: Advances in Neural Information\n",
      "\n",
      "Processing Systems, 1097–1105, 2012.\n",
      "\n",
      "[39] O. Abdel-Hamid, A.-R. Mohamed, H. Jiang, G. Penn, Applying convolutional neural networks concepts to hybrid NN-HMM model for speech\n",
      "\n",
      "recognition, in: 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 4277–4280, 2012.\n",
      "\n",
      "[40] Y. Kim, Convolutional neural networks for sentence classiﬁcation, arXiv preprint arXiv:1408.5882.\n",
      "[41] H. Jaeger, Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the ‘‘echo state network” approach, GMD-Forschungszentrum\n",
      "\n",
      "Informationstechnik, 2002.\n",
      "\n",
      "[42] C.L. Giles, C.B. Miller, D. Chen, H.-H. Chen, G.-Z. Sun, Y.-C. Lee, Learning and extracting ﬁnite state automata with second-order recurrent neural\n",
      "\n",
      "networks, Neural Comput. 4 (3) (1992) 393–405.\n",
      "\n",
      "[43] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Comput. 9 (8) (1997) 1735–1780.\n",
      "[44] F.A. Gers, J. Schmidhuber, F. Cummins, Learning to forget: continual prediction with LSTM, Neural Comput. 12 (10) (2000) 2451–2471.\n",
      "[45] F.A. Gers, N.N. Schraudolph, J. Schmidhuber, Learning precise timing with LSTM recurrent networks, J. Mach. Learn. Res. 3 (2002) 115–143.\n",
      "[46] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, Y. Bengio, Learning phrase representations using RNN encoder-\n",
      "\n",
      "decoder for statistical machine translation, arXiv preprint arXiv:1406.1078.\n",
      "\n",
      "[47] J. Chung, C. Gulcehre, K. Cho, Y. Bengio, Empirical evaluation of gated recurrent neural networks on sequence modeling, arXiv preprint\n",
      "\n",
      "arXiv:1412.3555.\n",
      "\n",
      "[48] S. Ruder, An overview of gradient descent optimization algorithms, arXiv preprint arXiv:1609.04747.\n",
      "[49] X. Glorot, Y. Bengio, Understanding the difﬁculty of training deep feedforward neural networks, in: Proceedings of the thirteenth international\n",
      "\n",
      "conference on artiﬁcial intelligence and statistics, 249–256, 2010.\n",
      "\n",
      "[50] H. Su, K.T. Chong, Induction machine condition monitoring using neural network modeling, IEEE Trans. Ind. Electron. 54 (1) (2007) 241–249, ISSN\n",
      "\n",
      "0278-0046.\n",
      "\n",
      "[51] B. Li, M.-Y. Chow, Y. Tipsuwan, J.C. Hung, Neural-network-based motor rolling bearing fault diagnosis, IEEE Trans. Industr. Electron. 47 (5) (2000)\n",
      "\n",
      "1060–1069.\n",
      "\n",
      "[52] B. Samanta, K. Al-Balushi, Artiﬁcial neural network based fault diagnostics of rolling element bearings using time-domain features, Mech. Syst. Signal\n",
      "\n",
      "Processing 17 (2) (2003) 317–328.\n",
      "\n",
      "[53] M. Aminian, F. Aminian, Neural-network based analog-circuit fault diagnosis using wavelet transform as preprocessor, IEEE Trans. Circuits Syst. II:\n",
      "\n",
      "Analog Digital Signal Processing 47 (2) (2000) 151–156.\n",
      "\n",
      "[54] W. Sun, S. Shao, R. Zhao, R. Yan, X. Zhang, X. Chen, A sparse auto-encoder-based deep neural network approach for induction motor faults\n",
      "\n",
      "classiﬁcation, Measurement 89 (2016) 171–178.\n",
      "\n",
      "[55] C. Lu, Z.-Y. Wang, W.-L. Qin, J. Ma, Fault diagnosis of rotary machinery components using a stacked denoising autoencoder-based health state\n",
      "\n",
      "identiﬁcation, Signal Processing 130 (2017) 377–388.\n",
      "\n",
      "[56] S. Tao, T. Zhang, J. Yang, X. Wang, W. Lu, Bearing fault diagnosis method based on stacked autoencoder and softmax regression, in: Control Conference\n",
      "\n",
      "(CCC), 2015 34th Chinese, IEEE, 6331–6335, 2015.\n",
      "\n",
      "[57] F. Jia, Y. Lei, J. Lin, X. Zhou, N. Lu, Deep neural networks: A promising tool for fault characteristic mining and intelligent diagnosis of rotating\n",
      "\n",
      "machinery with massive data, Mech. Syst. Signal Processing 72 (2016) 303–315.\n",
      "\n",
      "[58] J. Sun, C. Yan, J. Wen, Intelligent bearing fault diagnosis method combining compressed data acquisition and deep learning, IEEE Trans. Instrum. Meas.\n",
      "\n",
      "67 (1) (2018) 185–195.\n",
      "\n",
      "[59] F. Zhou, Y. Gao, C. Wen, A novel multimode fault classiﬁcation method based on deep learning, J. Control Sci. Eng. (2017) 14, Article ID 3583610.\n",
      "[60] T. Junbo, L. Weining, A. Juneng, W. Xueqian, Fault diagnosis method study in roller bearing based on wavelet transform and stacked auto-encoder, in:\n",
      "\n",
      "The 27th Chinese Control and Decision Conference (2015 CCDC), IEEE, 4608–4613, 2015.\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "235\n",
      "\n",
      "[61] Z. Huijie, R. Ting, W. Xinqing, Z. You, F. Husheng, Fault diagnosis of hydraulic pump based on stacked autoencoders, in: 2015 12th IEEE International\n",
      "\n",
      "Conference on Electronic Measurement Instruments (ICEMI), vol. 1, 58–62, 2015.\n",
      "\n",
      "[62] H. Liu, L. Li, J. Ma, Rolling Bearing Fault Diagnosis Based on STFT-Deep Learning and Sound Signals (2016), Shock and Vibration ArticleID 6127479, 12\n",
      "\n",
      "pages.\n",
      "\n",
      "[63] G.S. Galloway, V.M. Catterson, T. Fay, A. Robb, C. Love, Diagnosis of tidal turbine vibration data through deep neural networks, in: Proceedings of the\n",
      "\n",
      "3rd European Conference of the Prognostic and Health Management Society, PHM Society, 172–180, 2016.\n",
      "\n",
      "[64] K. Li, Q. Wang, Study on signal recognition and diagnosis for spacecraft based on deep learning method, in: Prognostics and System Health\n",
      "\n",
      "Management Conference (PHM), IEEE, 1–5, 2015.\n",
      "\n",
      "[65] L. Guo, H. Gao, H. Huang, X. He, S. Li, Multifeatures Fusion and Nonlinear Dimension Reduction for Intelligent Bearing Condition Monitoring (2016),\n",
      "\n",
      "Shock and Vibration ArticleID 4632562, 10pages.\n",
      "\n",
      "[66] N.K. Verma, V.K. Gupta, M. Sharma, R.K. Sevakula, Intelligent condition based monitoring of rotating machines using sparse auto-encoders, in: 2013\n",
      "\n",
      "IEEE Conference on Prognostics and Health Management (PHM), 1–7, 2013.\n",
      "\n",
      "[67] M. Sohaib, C.-H. Kim, J.-M. Kim, A hybrid feature model and deep-learning-based bearing fault diagnosis, Sensors 17 (12) (2017) 2876.\n",
      "[68] F. Cheng, J. Wang, L. Qu, W. Qiao, Rotor current-based fault diagnosis for DFIG wind turbine drivetrain gearboxes using frequency analysis and a deep\n",
      "\n",
      "classiﬁer, IEEE Trans. Ind. Appl. (2017).\n",
      "\n",
      "[69] R. Chen, S. Chen, M. He, D. He, B. Tang, Rolling bearing fault severity identiﬁcation using deep sparse auto-encoder network with noise added sample\n",
      "\n",
      "expansion, Proc. Inst. Mech. Eng., Part O: J. Risk Reliab. 231 (6) (2017) 666–679.\n",
      "\n",
      "[70] V.V. Kishore K. Reddy, Soumalya Sarkar, M. Giering, Anomaly Detection and Fault Disambiguation in Large Flight Data: A Multi-modal Deep Auto-\n",
      "\n",
      "encoder Approach, in: Annual Conference of the Prognostics and Health Management Society, Denver, Colorado, 1–8, 2016.\n",
      "\n",
      "[71] Z. Chen, W. Li, Multisensor feature fusion for bearing fault diagnosis using sparse autoencoder and deep belief network, IEEE Trans. Instrum. Meas. 66\n",
      "\n",
      "(7) (2017) 1693–1702.\n",
      "\n",
      "[72] R. Thirukovalluru, S. Dixit, R.K. Sevakula, N.K. Verma, A. Salour, Generating feature sets for fault diagnosis using denoising stacked auto-encoder, in:\n",
      "\n",
      "IEEE International Conference on Prognostics and Health Management (ICPHM), 1–7, 2016.\n",
      "\n",
      "[73] L. Wang, X. Zhao, J. Pei, G. Tang, Transformer fault diagnosis using continuous sparse autoencoder, SpringerPlus 5 (1) (2016) 1–13.\n",
      "[74] W. Mao, J. He, Y. Li, Y. Yan, Bearing fault diagnosis with auto-encoder extreme learning machine: A comparative study, Proc. Inst. Mech. Eng., Part C: J.\n",
      "\n",
      "Mech. Eng. Sci. 231 (8) (2017) 1560–1578.\n",
      "\n",
      "[75] E. Cambria, G.B. Huang, L.L.C. Kasun, H. Zhou, C.M. Vong, J. Lin, J. Yin, Z. Cai, Q. Liu, K. Li, V.C.M. Leung, L. Feng, Y.S. Ong, M.H. Lim, A. Akusok, A.\n",
      "Lendasse, F. Corona, R. Nian, Y. Miche, P. Gastaldo, R. Zunino, S. Decherchi, X. Yang, K. Mao, B.S. Oh, J. Jeon, K.A. Toh, A.B.J. Teoh, J. Kim, H. Yu, Y. Chen, J.\n",
      "Liu, Extreme learning machines [Trends Controversies], IEEE Intell. Syst. 28 (6) (2013) 30–59.\n",
      "\n",
      "[76] F. Jia, Y. Lei, L. Guo, J. Lin, S. Xing, A neural network constructed by deep learning technique and its application to intelligent fault diagnosis of\n",
      "\n",
      "machines, Neurocomputing 272 (2018) 619–628.\n",
      "\n",
      "[77] H. Shao, H. Jiang, H. Zhao, F. Wang, A novel deep autoencoder feature learning method for rotating machinery fault diagnosis, Mech. Syst. Signal\n",
      "\n",
      "Processing 95 (2017) 187–204.\n",
      "\n",
      "[78] H. Shao, H. Jiang, Y. Lin, X. Li, A novel method for intelligent fault diagnosis of rolling bearings using ensemble deep auto-encoders, Mech. Syst. Signal\n",
      "\n",
      "Processing 102 (2018) 278–297.\n",
      "\n",
      "[79] H. Shao, H. Jiang, F. Wang, H. Zhao, An enhancement deep feature fusion method for rotating machinery fault diagnosis, Knowl.-Based Syst. 119\n",
      "\n",
      "(2017) 200–220.\n",
      "\n",
      "[80] C. Li, W. Zhang, G. Peng, S. Liu, Bearing fault diagnosis using fully-connected winner-take-all autoencoder, IEEE Access (2017), https://doi.org/\n",
      "\n",
      "10.1109/ACCESS.2017.2717492.\n",
      "\n",
      "[81] W. Lu, X. Wang, C. Yang, T. Zhang, A novel feature extraction method using deep neural network for rolling bearing fault diagnosis, in: The 27th\n",
      "\n",
      "Chinese Control and Decision Conference (2015 CCDC), 2427–2431, 2015.\n",
      "\n",
      "[82] J. Deutsch, D. He, Using deep learning based approaches for bearing remaining useful life prediction, in: Annual Conference of the Prognostics and\n",
      "\n",
      "Health Management Society, PHM Society, 1–7, 2016.\n",
      "\n",
      "[83] J. Deutsch, D. He, Using deep learning-based approach to predict remaining useful life of rotating components, IEEE Trans. Syst., Man, Cybern.: Syst. 48\n",
      "\n",
      "(1) (2018) 11–20.\n",
      "\n",
      "[84] L. Liao, W. Jin, R. Pavel, Enhanced restricted boltzmann machine with prognosability regularization for prognostics and health assessment, IEEE Trans.\n",
      "\n",
      "Industr. Electron. 63 (11) (2016) 7076–7083.\n",
      "\n",
      "[85] C. Li, R.-V. Sanchez, G. Zurita, M. Cerrada, D. Cabrera, R.E. Vásquez, Multimodal deep support vector classiﬁcation with homologous features and its\n",
      "\n",
      "application to gearbox fault diagnosis, Neurocomputing 168 (2015) 119–127.\n",
      "\n",
      "[86] C. Li, R.-V. Sánchez, G. Zurita, M. Cerrada, D. Cabrera, Fault diagnosis for rotating machinery using vibration measurement deep statistical feature\n",
      "\n",
      "learning, Sensors 16 (6) (2016) 895.\n",
      "\n",
      "[87] C. Li, R.-V. Sanchez, G. Zurita, M. Cerrada, D. Cabrera, R.E. Vásquez, Gearbox fault diagnosis based on deep random forest fusion of acoustic and\n",
      "\n",
      "vibratory signals, Mech. Syst. Signal Processing 76 (2016) 283–293.\n",
      "\n",
      "[88] S.-Y. Shao, W.-J. Sun, R.-Q. Yan, P. Wang, R.X. Gao, A deep learning approach for fault diagnosis of induction motors in manufacturing, Chinese J. Mech.\n",
      "\n",
      "Eng. 30 (6) (2017) 1347–1356.\n",
      "\n",
      "[89] L. Zhang, H. Gao, J. Wen, S. Li, Q. Liu, A deep learning-based recognition method for degradation monitoring of ball screw with multi-sensor data\n",
      "\n",
      "fusion, Microelectron. Reliab. 75 (2017) 215–222.\n",
      "\n",
      "[90] X. Wang, J. Huang, G. Ren, D. Wang, A hydraulic fault diagnosis method based on sliding-window spectrum feature and deep belief network, J.\n",
      "\n",
      "Vibroengineering 19 (6) (2017) 4272–4284.\n",
      "\n",
      "[91] D. Han, N. Zhao, P. Shi, A new fault diagnosis method based on deep belief network and support vector machine with Teager-Kaiser energy operator\n",
      "\n",
      "for bearings, Adv. Mech. Eng. 9 (12) (2017), 1687814017743113.\n",
      "\n",
      "[92] P. Wang, R.X. Gao, R. Yan, A deep learning-based approach to material removal rate prediction in polishing, CIRP Ann.-Manuf. Technol. 66 (1) (2017)\n",
      "\n",
      "429–432.\n",
      "\n",
      "[93] Z. Chen, S. Deng, X. Chen, C. Li, R.-V. Sanchez, H. Qin, Deep neural networks-based rolling bearing fault diagnosis, Microelectron. Reliab. 75 (2017)\n",
      "\n",
      "327–333.\n",
      "\n",
      "[94] Z. Gao, C. Ma, D. Song, Y. Liu, Deep quantum inspired neural network with application to aircraft fuel system fault diagnosis, Neurocomputing 238\n",
      "\n",
      "(2017) 13–23.\n",
      "\n",
      "[95] H. Oh, J.H. Jung, B.C. Jeon, B.D. Youn, Scalable and unsupervised feature engineering using vibration-imaging and deep learning for rotor system\n",
      "\n",
      "diagnosis, IEEE Trans. Industr. Electron. 65 (4) (2018) 3539–3549.\n",
      "\n",
      "[96] M. Ma, X. Chen, S. Wang, Y. Liu, W. Li, Bearing degradation assessment based on weibull distribution and deep belief network, in: Proceedings of 2016\n",
      "\n",
      "International Symposium of Flexible Automation (ISFA), 1–4, 2016.\n",
      "\n",
      "[97] S. Shao, W. Sun, P. Wang, R.X. Gao, R. Yan, Learning features from vibration signals for induction motor fault diagnosis, in: Proceedings of 2016\n",
      "\n",
      "International Symposium of Flexible Automation (ISFA), 1–6, 2016.\n",
      "\n",
      "[98] Y. Fu, Y. Zhang, H. Qiao, D. Li, H. Zhou, J. Leopold, Analysis of feature extracting ability for cutting state monitoring using deep belief networks,\n",
      "\n",
      "Procedia CIRP 31 (2015) 29–34.\n",
      "\n",
      "[99] P. Tamilselvan, P. Wang, Failure diagnosis using deep belief learning based health state classiﬁcation, Reliab. Eng. Syst. Saf. 115 (2013) 124–135.\n",
      "[100] P. Tamilselvan, Y. Wang, P. Wang, Deep belief network based state classiﬁcation for structural health diagnosis, in: 2012 IEEE Aerospace Conference,\n",
      "\n",
      "1–11, 2012.\n",
      "\n",
      "\f",
      "236\n",
      "\n",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "[101] J. Tao, Y. Liu, D. Yang, Bearing fault diagnosis based on deep belief network and multisensor information fusion, Shock Vib. (2016) 9, Article ID\n",
      "\n",
      "9306205.\n",
      "\n",
      "[102] Z. Chen, C. Li, R.-V. Sánchez, Multi-layer neural network with deep belief network for gearbox fault diagnosis, J. Vibroeng. 17 (5) (2015) 2379–2392.\n",
      "[103] M. Gan, C. Wang, et al, Construction of hierarchical diagnosis network based on deep learning and its application in the fault pattern recognition of\n",
      "\n",
      "rolling element bearings, Mech. Syst. Signal Processing 72 (2016) 92–104.\n",
      "\n",
      "[104] H. Oh, B.C. Jeon, J.H. Jung, B.D. Youn, Unsupervised feature extraction scheme by deep learning, in: Annual Conference of the Prognostic and Health\n",
      "\n",
      "Management Society, PHM Society, 1–8, 2016.\n",
      "\n",
      "[105] C. Zhang, J.H. Sun, K.C. Tan, Deep belief networks ensemble with multi-objective optimization for failure diagnosis, in: Systems, Man, and Cybernetics\n",
      "\n",
      "(SMC), 2015 IEEE International Conference on, IEEE, 32–37, 2015.\n",
      "\n",
      "[106] C. Zhang, P. Lim, A. Qin, K.C. Tan, Multiobjective deep belief networks ensemble for remaining useful life estimation in prognostics, IEEE Trans. Neural\n",
      "\n",
      "Networks Learn. Syst. 28 (10) (2017) 2306–2318.\n",
      "\n",
      "[107] R. Liu, G. Meng, B. Yang, C. Sun, X. Chen, Dislocated time series convolutional neural architecture: an intelligent fault diagnosis approach for electric\n",
      "\n",
      "machine, IEEE Trans. Industr. Inf. 13 (2) (2017) 1310–1320.\n",
      "\n",
      "[108] P. Wang, Ananya, R. Yan, R.X. Gao, Virtualization and deep recognition for System Fault Classiﬁcation, J. Manuf. Syst. 44 (2017) 310–316.\n",
      "[109] O. Janssens, V. Slavkovikj, B. Vervisch, K. Stockman, M. Loccuﬁer, S. Verstockt, R. Van de Walle, S. Van Hoecke, Convolutional neural network based\n",
      "\n",
      "fault detection for rotating machinery, J. Sound Vib. 377 (2016) 331–345.\n",
      "\n",
      "[110] C. Lu, Z. Wang, B. Zhou, Intelligent fault diagnosis of rolling bearing using hierarchical convolutional network based health state classiﬁcation, Adv.\n",
      "\n",
      "Eng. Inform. 32 (2017) 139–151.\n",
      "\n",
      "[111] G.S. Babu, P. Zhao, X.-L. Li, Deep convolutional neural network based regression approach for estimation of remaining useful life, in: International\n",
      "\n",
      "Conference on Database Systems for Advanced Applications, Springer, 214–228, 2016.\n",
      "\n",
      "[112] X. Ding, Q. He, Energy-ﬂuctuated multiscale feature learning with deep convnet for intelligent spindle bearing fault diagnosis, IEEE Trans. Instrum.\n",
      "\n",
      "Meas. 66 (8) (2017) 1926–1935.\n",
      "\n",
      "[113] X. Guo, L. Chen, C. Shen, Hierarchical adaptive deep convolution neural network and its application to bearing fault diagnosis, Measurement 93 (2016)\n",
      "\n",
      "490–502.\n",
      "\n",
      "[114] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, Proc. IEEE 86 (11) (1998) 2278–2324.\n",
      "[115] W. Sun, B. Yao, N. Zeng, B. Chen, Y. He, X. Cao, W. He, An intelligent gear fault diagnosis methodology using a complex wavelet enhanced\n",
      "\n",
      "convolutional neural network, Materials 10 (7) (2017) 790.\n",
      "\n",
      "[116] J. Wang, j. Zhuang, L. Duan, W. Cheng, A multi-scale convolution neural network for featureless fault diagnosis, in: Proceedings of 2016 International\n",
      "\n",
      "Symposium of Flexible Automation (ISFA), 1–6, 2016c.\n",
      "\n",
      "[117] Z. Chen, C. Li, R.-V. Sanchez, Gearbox fault identiﬁcation and classiﬁcation with convolutional neural networks, Shock Vib (2015) 10, Atricle ID\n",
      "\n",
      "390134.\n",
      "\n",
      "[118] D. Weimer, B. Scholz-Reiter, M. Shpitalni, Design of deep convolutional neural network architectures for automated feature extraction in industrial\n",
      "\n",
      "inspection, CIRP Annals-Manufacturing Technology.\n",
      "\n",
      "[119] H.-Y. Dong, L.-X. Yang, H.-W. Li, Small fault diagnosis of front-end speed controlled wind generator based on deep learning, WSEAS Trans. Circuits\n",
      "\n",
      "Syst. 15 (2016) 64–72.\n",
      "\n",
      "[120] W. You, C. Shen, X. Guo, X. Jiang, J. Shi, Z. Zhu, A hybrid technique based on convolutional neural network and support vector regression for intelligent\n",
      "\n",
      "diagnosis of rotating machinery, Adv. Mech. Eng. 9 (6) (2017) 168–176.\n",
      "\n",
      "[121] K.B. Lee, S. Cheon, C.O. Kim, A convolutional neural network for fault classiﬁcation and diagnosis in semiconductor manufacturingprocesses, IEEE\n",
      "\n",
      "Trans. Semicond. Manuf. 30 (2) (2017) 135–142.\n",
      "\n",
      "[122] L. Wen, X. Li, L. Gao, Y. Zhang, A new convolutional neural network based data-driven fault diagnosis method, IEEE Transactions on Industrial\n",
      "\n",
      "Electronics doi10.1109/TIE.2017.2774777.\n",
      "\n",
      "[123] S. Li, G. Liu, X. Tang, J. Lu, J. Hu, An ensemble deep convolutional neural network model with improved DS evidence fusion for bearing fault diagnosis,\n",
      "\n",
      "Sensors 17 (8) (2017) 1729.\n",
      "\n",
      "[124] S.K. Singh, S. Kumar, J. Dwivedi, Compound fault prediction of rolling bearing using multimedia data, Multimedia Tools Appl. (2017) 1–18.\n",
      "[125] D. Verstraete, A. Ferrada, E.L. Droguett, V. Meruane, M. Modarres, Deep learning enabled fault diagnosis using time-frequency image analysis of\n",
      "\n",
      "rolling element bearings, Shock and Vibration (2017) 17, AtricleID 506765.\n",
      "\n",
      "[126] T. Ince, S. Kiranyaz, L. Eren, M. Askar, M. Gabbouj, Real-time motor fault detection by 1-D convolutional neural networks, IEEE Trans. Industr. Electron.\n",
      "\n",
      "63 (11) (2016) 7067–7075.\n",
      "\n",
      "[127] O. Abdeljaber, O. Avci, S. Kiranyaz, M. Gabbouj, D.J. Inman, Real-time vibration-based structural damage detection using one-dimensional\n",
      "\n",
      "convolutional neural networks, J. Sound Vib. 388 (2017) 154–170.\n",
      "\n",
      "[128] L. Jing, M. Zhao, P. Li, X. Xu, A convolutional neural network based feature learning and fault diagnosis method for the condition monitoring of\n",
      "\n",
      "gearbox, Measurement 111 (2017) 1–10.\n",
      "\n",
      "[129] W. Zhang, C. Li, G. Peng, Y. Chen, Z. Zhang, A deep convolutional neural network with new training methods for bearing fault diagnosis under noisy\n",
      "\n",
      "environment and different working load, Mech. Syst. Signal Processing 100 (2018) 439–453.\n",
      "\n",
      "[130] W. Zhang, G. Peng, C. Li, Y. Chen, Z. Zhang, A new deep learning model for fault diagnosis with good anti-noise and domain adaptation ability on raw\n",
      "\n",
      "vibration signals, Sensors 17 (2) (2017) 425.\n",
      "\n",
      "[131] Y. Li, N. Wang, J. Shi, J. Liu, X. Hou, Revisiting batch normalization for practical domain adaptation, arXiv preprint arXiv:1603.04779.\n",
      "[132] W. Sun, R. Zhao, R. Yan, S. Shao, X. Chen, Convolutional discriminative feature learning for induction motor fault diagnosis, IEEE Trans. Industr. Inf. 13\n",
      "\n",
      "(3) (2017) 1350–1359.\n",
      "\n",
      "[133] D. Cabrera, F. Sancho, C. Li, M. Cerrada, R.-V. Sánchez, F. Pacheco, J.V. de Oliveira, Automatic feature extraction of time-series applied to fault severity\n",
      "\n",
      "assessment of helical gearbox in stationary and non-stationary speed operation, Appl. Soft Comput. 58 (2017) 53–64.\n",
      "\n",
      "[134] H. Shao, H. Jiang, H. Zhang, T. Liang, Electric locomotive bearing fault diagnosis using novel convolutional deep belief network, IEEE Trans. Industr.\n",
      "\n",
      "Electron. 65 (5) (2018) 4290–4300.\n",
      "\n",
      "[135] M. Zhao, M. Kang, B. Tang, M. Pecht, Deep residual networks with dynamically weighted wavelet coefﬁcients for fault diagnosis of planetary\n",
      "\n",
      "gearboxes, IEEE Trans. Ind. Electron. (2017).\n",
      "\n",
      "[136] J. Pan, Y. Zi, J. Chen, Z. Zhou, B. Wang, LiftingNet: a novel deep learning network with layerwise feature learning from noisy mechanical data for fault\n",
      "\n",
      "classiﬁcation, IEEE Trans. Ind. Electron. (2017), https://doi.org/10.1109/TIE.2017.2767540.\n",
      "\n",
      "[137] M. Yuan, Y. Wu, L. Lin, Fault diagnosis and remaining useful life estimation of aero engine using LSTM neural network, in: 2016 IEEE International\n",
      "\n",
      "Conference on Aircraft Utility Systems (AUS), 135–140, 2016.\n",
      "\n",
      "[138] R. Zhao, J. Wang, R. Yan, K. Mao, Machine health monitoring with LSTM networks, in: IEEE International Conference on Sensing Technology, 1–6, 2016.\n",
      "[139] R. Zhao, R. Yan, J. Wang, K. Mao, Learning to monitor machine health with convolutional bi-directional lstm networks, Sensors 17 (2) (2017) 273.\n",
      "[140] R. Zhao, D. Wang, R. Yan, K. Mao, F. Shen, J. Wang, Machine health monitoring using local feature-based gated recurrent unit networks, IEEE Trans.\n",
      "\n",
      "Industr. Electron. 65 (2) (2017) 1539–1548.\n",
      "\n",
      "[141] P. Malhotra, A. Ramakrishnan, G. Anand, L. Vig, P. Agarwal, G. Shroff, Multi-sensor prognostics using an unsupervised health index based on LSTM\n",
      "\n",
      "Encoder-Decoder, arXiv preprint arXiv:1608.06154.\n",
      "\n",
      "[142] V. Stojanovic, N. Nedic, Robust identiﬁcation of OE model with constrained output using optimal input design, J. Franklin Inst. 353 (2) (2016) 576–\n",
      "\n",
      "593.\n",
      "\n",
      "[143] V. Stojanovic, V. Filipovic, Adaptive input design for identiﬁcation of output error model with constrained output, Circuits, Systems, Signal Processing\n",
      "\n",
      "33 (1) (2014) 97–113.\n",
      "\n",
      "\f",
      "R. Zhao et al. / Mechanical Systems and Signal Processing 115 (2019) 213–237\n",
      "\n",
      "237\n",
      "\n",
      "[144] V. Filipovic, N. Nedic, V. Stojanovic, Robust identiﬁcation of pneumatic servo actuators in the real situationsRobuste Identiﬁkation von\n",
      "\n",
      "pneumatischen Servo-Aktuatoren in der realen Situationen, Forsch. Ingenieurwes. 75 (4) (2011) 183–196.\n",
      "\n",
      "[145] phm society, 2010 PHM Data Challenge, URLhttps://www.phmsociety.org/competition/phm/10, 2010\n",
      "[146] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, arXiv preprint arXiv:1512.03385.\n",
      "[147] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L.F.-F., ImageNet: A Large-Scale Hierarchical Image Database, in: 2009 IEEE Conference on Computer Vision\n",
      "\n",
      "and Pattern Recognition, 248–255, 2009.\n",
      "\n",
      "[148] L.v.d. Maaten, G. Hinton, Visualizing data using t-SNE, J. Mach. Learn. Res. 9 (2008) 2579–2605.\n",
      "[149] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, H. Lipson, Understanding neural networks through deep visualization, arXiv preprint arXiv:1506.06579.\n",
      "[150] F. Shen, C. Chen, R. Yan, R.X. Gao, Bearing fault diagnosis based on SVD feature extraction and transfer learning classiﬁcation, in: 2015 IEEE\n",
      "\n",
      "Prognostics and System Health Management Conference (PHM), 1–6, 2015.\n",
      "\n",
      "[151] J. Xie, L. Zhang, L. Duan, J. Wang, On cross-domain feature fusion in gearbox fault diagnosis under various operating conditions based on Transfer\n",
      "\n",
      "Component Analysis, in: IEEE International Conference on Prognostics and Health Management (ICPHM), 1–6, 2016.\n",
      "\n",
      "[152] W. Lu, B. Liang, Y. Cheng, D. Meng, J. Yang, T. Zhang, Deep model based domain adaptation for fault diagnosis, IEEE Trans. Industr. Electron. 64 (3)\n",
      "\n",
      "(2017) 2296–2305.\n",
      "\n",
      "[153] W. Mao, L. He, Y. Yan, J. Wang, Online sequential prediction of bearings imbalanced fault diagnosis by extreme learning machine, Mech. Syst. Signal\n",
      "\n",
      "Processing 83 (2017) 450–473.\n",
      "\n",
      "[154] L. Duan, M. Xie, T. Bai, J. Wang, A new support vector data description method for machinery fault diagnosis with unbalanced datasets, Expert Syst.\n",
      "\n",
      "Appl. 64 (2016) 239–246.\n",
      "\n",
      "[155] C. Huang, Y. Li, C. Change Loy, X. Tang, Learning deep representation for imbalanced classiﬁcation, in: Proceedings of the IEEE Conference on\n",
      "\n",
      "Computer Vision and Pattern Recognition, 5375–5384, 2016.\n",
      "\n",
      "[156] Y. Yan, M. Chen, M.-L. Shyu, S.-C. Chen, Deep Learning for Imbalanced Multimedia Data Classiﬁcation, in: 2015 IEEE International Symposium on\n",
      "\n",
      "Multimedia (ISM), IEEE, 483–488, 2015.\n",
      "\n",
      "View publication stats\n",
      "View publication stats\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "\n",
    "\n",
    "fp = open('t.pdf', 'rb')\n",
    "rsrcmgr = PDFResourceManager()\n",
    "retstr = io.StringIO()\n",
    "codec = 'utf-8'\n",
    "laparams = LAParams()\n",
    "device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "# Create a PDF interpreter object.\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "# Process each page contained in the document.\n",
    "\n",
    "for page in PDFPage.get_pages(fp):\n",
    "    interpreter.process_page(page)\n",
    "    data =  retstr.getvalue()\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = ' '\n",
    "stopwords = set(STOPWORDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6fc274424c53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# typecaste each val to string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "for val in df.CONTENT: \n",
    "      \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "          \n",
    "    for words in tokens: \n",
    "        comment_words = comment_words + words + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
